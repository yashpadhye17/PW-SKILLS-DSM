{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 MARCH ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lack of a value or data point in one or more variables or aspects of an observation is referred to as missing values in a dataset. Missing values can arise for a variety of causes, including data input mistakes, sensor problems, or survey non-response. Missing values must be handled carefully since they can impair the quality, reliability, and validity of data analysis and modelling outputs.\n",
    "\n",
    "Some common reasons why it is important to handle missing values in a dataset are:\n",
    "\n",
    "* Missing values can lower sample size and statistical power, resulting in erroneous estimates and conclusions.\n",
    "* Missing values, such as correlations or distributions, can add mistakes or inconsistencies into the data analysis or modelling process.\n",
    "* Missing values can create computational mistakes or biases in methods that expect complete or valid data, such as regression or clustering.\n",
    "\n",
    "The following algorithms are unaffected by missing values:\n",
    "\n",
    "* Decision trees and random forests can deal with missing values by disregarding them when splitting nodes or imputing them depending on the majority class or mean of the samples.\n",
    "* Naive Bayes:Missing values are handled by disregarding them while calculating the prior and likelihood probabilities, assuming that the missingness is random.\n",
    "* K-nearest neighbours (KNN): This approach can handle missing values by imputed the missing values using the available values of the nearest neighbours."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q2: List down techniques used to handle missing data. Give an example of each with python code.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several techniques used to handle missing data. Some of them are:\n",
    "\n",
    "Deletion: In this technique, we remove the rows or columns containing missing values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df=sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation: We use this approach to fill in the missing data. Missing value impute methods include mean imputation, median imputation, mode imputation, and regression imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) imputation: In this technique, we fill the missing values with the values of the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df['age'] = imputer.fit_transform(df[['age']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data occurs when the distribution of classes in a dataset is not equal. In a binary classification task, for example, if one class contains considerably more samples than the other, the data is termed unbalanced. Imbalanced data can lead to biassed models, in which the model predicts the majority class while disregarding the minority class.\n",
    "\n",
    "If unbalanced data is not handled properly, the model's performance might suffer and results can be deceptive. In the instance of medical diagnosis, if the dataset is uneven and the model is biassed towards the majority class, the minority class may miss diagnoses. Similarly, if the model is biassed against the majority class, it might result in numerous false negatives, missing the fraudulent transactions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down-sampling is the process of lowering the size of the majority class by randomly eliminating some of its samples. Assume we have a dataset of 1000 samples, 900 of which belong to the majority class and 100 of which belong to the minority class. In such instance, we can balance the dataset by randomly removing some of the majority class samples.\n",
    "\n",
    "Up-sampling, on the other hand, entails reproducing the minority class's samples in order to increase the size of the minority class. For example, we can duplicate the minority class samples in the same dataset as before to balance the dataset.\n",
    "\n",
    "The decision between upsampling and downsampling is influenced by the dataset and the situation at hand. Down-sampling might result in information loss if the dataset is tiny and comprises a small number of samples. Down-sampling, on the other hand, can be a valuable strategy for balancing a huge dataset.\n",
    "\n",
    "Up-sampling, on the other hand, can be utilised when we have a small number of minority class samples and wish to enhance their representation in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q5: What is data Augmentation? Explain SMOTE.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a machine learning approach that creates new synthetic samples from the original data to artificially enhance the size of the training dataset. It is widely employed to solve the issue of overfitting and to improve the model's generalisation capacity.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation approach designed to solve the issue of unbalanced datasets. It generates new synthetic minority class samples by interpolating between existing minority class samples. Interpolation is performed by choosing two or more examples of the same minority class and producing new instances along the line segments that link them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q6: What are outliers in a dataset? Why is it essential to handle outliers?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are values that differ greatly from the rest of the observations in a dataset. Outliers must be handled carefully since they can skew the outcomes of data analysis and statistical modelling, resulting in skewed estimates and inaccurate judgements. Outliers can happen owing to several factors, such as measurement mistakes, data entry problems, or true but extreme numbers.\n",
    "\n",
    "Outliers can have a considerable influence on statistical measurements like mean and standard deviation, rendering them untrustworthy for characterising data. Outliers can also have an impact on the effectiveness of machine learning algorithms, especially those that are sensitive to data distribution, such as clustering and distance-based methods.\n",
    "\n",
    "Depending on the nature of the data and the purpose of the research, dealing with outliers may require a variety of strategies. Outliers are commonly dealt with by deleting them from the dataset, changing the data with mathematical functions, or replacing them with more appropriate values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous approaches for dealing with missing data in a dataset:\n",
    "\n",
    "Deletion: With this strategy, we simply eliminate the rows or columns from the dataset that have missing data. If the amount of missing data is modest and randomly distributed, this strategy can be employed. Unfortunately, this approach may result in information loss and a smaller sample size.\n",
    "\n",
    "Mean/Mode/Median Imputation: With this approach, we replace missing values with the feature's mean, mode, or median value. This approach is quick and easy to use, but it might provide biassed findings and underestimate the standard error.\n",
    "\n",
    "Regression Imputation: In this method, we use a regression model to forecast missing data. Although this approach produces correct imputed numbers, it is computationally intensive.\n",
    "\n",
    "\n",
    "Multiple imputation: With this technique, we build numerous imputed datasets, where the missing values are imputed using different methods, and then combine the results. This approach can produce accurate imputed values as well as uncertainty estimations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous ways that may be used to identify whether the missing data is missing at random or whether it has a pattern:\n",
    "\n",
    "Visual inspection: Plotting the data and looking for patterns in the missing numbers might be a good place to start. Missing data, for example, may occur in blocks, suggesting that they are associated with a certain variable or time period.\n",
    "\n",
    "Imputation and comparison: Another option is to use multiple imputation or other techniques to fill in the missing data, and then compare the outcomes of the analysis with and without the imputed values. If the findings are comparable, it implies that the missing data is missing at random, however if the results differ greatly, it suggests that the missing data is missing at random.\n",
    "\n",
    "Expert knowledge: Based on expert knowledge of the data and the environment in which it was acquired, it may be feasible to identify whether the missing data is absent at random or not in some circumstances.\n",
    "\n",
    "\n",
    "It is vital to evaluate whether the missing data is missing at random or not since this might impact the validity and reliability of the study. If the missing data is not absent at random, it may inject bias into the study and alter the conclusions made from the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous methods for evaluating the performance of a machine learning model on an unbalanced dataset:\n",
    "\n",
    "A confusion matrix may be used to evaluate a model's performance by comparing anticipated and actual values of the target variable. It may be used to compute metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "\n",
    "Precision and recall are two significant measures that are typically used to evaluate a model's performance on an unbalanced dataset. Precision is the proportion of correct positive predictions among all correct positive predictions, whereas recall is the fraction of correct positive predictions among all correct positives.\n",
    "\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall, and it is frequently used to assess a model's performance on an unbalanced dataset. It combines accuracy and recall and produces a single score that summarises the model's performance.\n",
    "\n",
    "The ROC curve (Receiver Operating Characteristic curve) and AUC: The ROC curve (Receiver Operating Characteristic curve) is a depiction of the true positive rate (TPR) versus the false positive rate (FPR) at various categorization thresholds. AUC (Area Under the Curve) is the area under the ROC curve, which is frequently used to assess a model's performance on an unbalanced dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use the following ways to balance the dataset and down-sample the majority class:\n",
    "\n",
    "Random under-sampling: To balance the dataset, you randomly eliminate samples from the dominant class. If key samples are eliminated, this procedure may result in data loss.\n",
    "\n",
    "Random over-sampling: To balance the dataset, you randomly duplicate samples from the minority class. If duplicates are made without considering the distribution of the data, this strategy may result in overfitting.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): This approach generates fresh samples from the minority class by interpolating between existing samples. Because it creates fresh samples based on the underlying distribution of the data, this strategy is less prone to overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some typical approaches for up-sampling the minority class in an unbalanced dataset are:\n",
    "\n",
    "* Random Over-Sampling: Duplicating minority class samples at random in order to boost their representation in the dataset.\n",
    "* SMOTE (Synthetic Minority Over-sampling Technique): This approach generates synthetic minority samples by taking a sample and generating new samples that are linear combinations of the original sample and its nearest neighbours.\n",
    "* Adaptive Synthetic Sampling (ADASYN): This approach similarly generates synthetic samples of the minority class, but it focuses on the minority samples that are more difficult to learn by creating more synthetic samples for those that are more difficult to learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
