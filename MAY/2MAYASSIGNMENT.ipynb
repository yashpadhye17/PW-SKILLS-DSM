{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 MAY ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the norm or expected behavior within a dataset. The goal of anomaly detection is to distinguish these rare and unusual data points, often referred to as anomalies or outliers, from the majority of the data that follows typical patterns. Anomalies can take various forms, such as unexpected spikes, sudden drops, unusual patterns, or outliers in the data distribution.\n",
    "\n",
    "The purpose of anomaly detection is to uncover and flag data points that are different from the norm, which can have several practical applications:\n",
    "\n",
    "1. Fault detection: Anomaly detection is commonly used in various domains, such as manufacturing, network monitoring, or system maintenance, to identify faulty or malfunctioning equipment or processes. By detecting anomalies, companies can quickly respond to and address issues, minimizing downtime and optimizing operations.\n",
    "\n",
    "2. Intrusion detection: In the realm of cybersecurity, anomaly detection plays a crucial role in identifying anomalous or suspicious activities that may indicate a security breach or unauthorized access attempts. By analyzing network traffic or user behavior, anomalies can be detected and prompt action can be taken to mitigate potential threats.\n",
    "\n",
    "3. Fraud detection: Anomaly detection is valuable in detecting fraudulent activities, such as credit card fraud, insurance fraud, or identity theft. Unusual spending patterns, atypical transactions, or deviations from established behavioral profiles can be flagged as potential fraudulent behavior, allowing timely intervention and prevention of financial losses.\n",
    "\n",
    "4. Performance monitoring: Anomaly detection can be applied to monitor the performance of complex systems or processes. By continuously analyzing relevant metrics, deviations from expected performance levels can be identified, enabling proactive maintenance or optimization to prevent failures or degradation.\n",
    "\n",
    "5. Health monitoring: Anomaly detection techniques are used in healthcare to identify abnormal patient conditions or disease outbreaks. By monitoring vital signs, patient data, or population health trends, anomalies can be detected, leading to early diagnosis, timely intervention, and better healthcare management.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection poses several challenges that can make the task complex and demanding. Some of the key challenges in anomaly detection include:\n",
    "\n",
    "1. Lack of labeled data: Anomaly detection often requires labeled data, where anomalies are explicitly identified and labeled. However, obtaining labeled data can be difficult and expensive, as anomalies are typically rare and diverse. Annotating anomalies in large datasets may also be subjective, as what constitutes an anomaly can vary depending on the context or domain.\n",
    "\n",
    "2. Imbalanced data: Anomalies are usually a small fraction of the overall dataset, resulting in imbalanced data distributions. Traditional machine learning algorithms tend to be biased towards the majority class, making it challenging to effectively capture and detect anomalies. Specialized techniques, such as sampling methods or anomaly detection algorithms designed for imbalanced data, are needed to address this challenge.\n",
    "\n",
    "3. Dynamic and evolving anomalies: Anomalies can change over time as systems, processes, or behaviors evolve. Static anomaly detection models may struggle to adapt to new types of anomalies or shifting patterns. Continuous monitoring and updating of anomaly detection models are required to handle evolving anomalies effectively.\n",
    "\n",
    "4. High-dimensional data: Many real-world datasets contain a large number of features or dimensions, making it challenging to detect anomalies in high-dimensional spaces. The curse of dimensionality can lead to increased computational complexity, decreased detection accuracy, and difficulty in visualizing and interpreting results. Dimensionality reduction techniques or specialized algorithms designed for high-dimensional data can help address this challenge.\n",
    "\n",
    "5. Noise and data quality: Real-world datasets often contain noise, errors, or missing values, which can complicate anomaly detection. Noise can obscure the true anomalies or introduce false positives, while missing or erroneous data can affect the accuracy of anomaly detection algorithms. Data preprocessing steps, such as data cleaning and imputation, are necessary to handle these issues and ensure reliable anomaly detection.\n",
    "\n",
    "6. Contextual and subjective anomalies: Anomalies are highly contextual and can vary depending on the application domain or specific use case. Determining what constitutes an anomaly requires domain knowledge and expertise, and anomalies may be subjective or context-dependent. Incorporating domain-specific knowledge and contextual information into anomaly detection algorithms is crucial to improve accuracy and relevance.\n",
    "\n",
    "7. Scalability and real-time detection: As datasets grow larger and real-time anomaly detection becomes more critical, scalability becomes a challenge. Anomaly detection algorithms must be able to handle big data efficiently and provide real-time or near-real-time results. Stream processing techniques and scalable algorithms are necessary to address the scalability challenge.\n",
    "\n",
    "Addressing these challenges requires a combination of advanced machine learning techniques, domain expertise, and a thorough understanding of the specific application context to develop effective anomaly detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two different approaches to detecting anomalies in a dataset. Here's how they differ:\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "Unsupervised anomaly detection is a technique used to detect anomalies in a dataset without the need for labeled data or prior knowledge of anomalies. In unsupervised learning, the algorithm learns the normal patterns or structure of the data and identifies instances that deviate significantly from that norm as anomalies. The algorithm explores the inherent characteristics and patterns in the data to identify outliers or unusual data points.\n",
    "\n",
    "Key characteristics of unsupervised anomaly detection include:\n",
    "1. No labeled anomalies: Unsupervised methods do not require prior knowledge or labeled examples of anomalies during the training phase.\n",
    "2. Discovery-based approach: Unsupervised algorithms aim to discover patterns, clusters, or density regions in the data and identify instances that do not conform to those patterns.\n",
    "3. Lack of anomaly definition: Unsupervised methods do not assume a specific definition of what constitutes an anomaly. Instead, they focus on identifying deviations from the normal data distribution.\n",
    "4. Broad applicability: Unsupervised anomaly detection techniques can be applied to a wide range of datasets and domains where labeled anomaly data is scarce or unavailable.\n",
    "\n",
    "Supervised Anomaly Detection:\n",
    "Supervised anomaly detection, on the other hand, relies on labeled data that explicitly identifies anomalies during the training phase. In this approach, the algorithm learns from labeled examples of both normal and anomalous instances, building a model that can classify future data points as either normal or anomalous.\n",
    "\n",
    "Key characteristics of supervised anomaly detection include:\n",
    "1. Labeled anomalies: Supervised methods require a labeled training dataset where anomalies are explicitly marked or labeled.\n",
    "2. Classification-based approach: Supervised algorithms learn to classify data points as normal or anomalous based on the provided labels.\n",
    "3. Clear anomaly definition: Supervised methods assume a specific definition of what constitutes an anomaly based on the provided labels.\n",
    "4. Domain-specific knowledge: Supervised anomaly detection often relies on domain expertise to label anomalies accurately and generalize the learned model to new instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be categorized into several main types based on their underlying principles and techniques. Here are some of the main categories:\n",
    "\n",
    "1. Statistical Methods: Statistical-based anomaly detection algorithms assume that the data follows a known statistical distribution. They use statistical measures such as mean, standard deviation, or probability distributions (e.g., Gaussian distribution) to identify data points that deviate significantly from the expected patterns.\n",
    "\n",
    "2. Machine Learning Methods:\n",
    "   a. Unsupervised Learning: Unsupervised anomaly detection algorithms learn the normal patterns or structure of the data without prior knowledge of anomalies. They use techniques like clustering, density estimation, or dimensionality reduction (e.g., k-means, DBSCAN, Isolation Forest) to identify instances that deviate from the normal data distribution.\n",
    "   b. Supervised Learning: Supervised anomaly detection algorithms rely on labeled data where anomalies are explicitly identified during the training phase. They build a model that can classify new data points as normal or anomalous based on the provided labels. Techniques such as classification algorithms (e.g., Support Vector Machines, Random Forests) are commonly used in supervised anomaly detection.\n",
    "\n",
    "3. Distance-based Methods: Distance-based anomaly detection algorithms measure the similarity or dissimilarity between data points and use distance metrics (e.g., Euclidean distance, Mahalanobis distance) to identify instances that are far from the majority of the data.\n",
    "\n",
    "4. Clustering Methods: Clustering-based anomaly detection algorithms group similar data points together and consider data points that do not belong to any cluster or form small clusters as anomalies. Anomalies are considered as data points that have a low density or do not conform to any existing cluster structure.\n",
    "\n",
    "5. Density Estimation Methods: Density-based anomaly detection algorithms estimate the density of the data and identify instances that fall in low-density regions as anomalies. Techniques like kernel density estimation or local outlier factor (LOF) are commonly used in density-based anomaly detection.\n",
    "\n",
    "6. Neural Network-based Methods: Neural network-based anomaly detection algorithms use neural network architectures, such as autoencoders or recurrent neural networks (RNNs), to learn the normal patterns in the data. They aim to reconstruct the input data and identify instances with high reconstruction errors as anomalies.\n",
    "\n",
    "7. Ensemble Methods: Ensemble anomaly detection algorithms combine multiple anomaly detection techniques or models to improve the accuracy and robustness of the detection. They leverage the diversity of different algorithms to achieve better anomaly detection performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on certain assumptions about the data and the characteristics of anomalies. Here are the main assumptions made by distance-based anomaly detection methods:\n",
    "\n",
    "1. Normal Data Distribution: Distance-based methods assume that the majority of the data points follow a specific distribution or exhibit a certain pattern. Typically, it is assumed that normal data points are tightly clustered around each other, forming regions of high density.\n",
    "\n",
    "2. Distance Measure: These methods assume the availability of a distance metric to measure the similarity or dissimilarity between data points. Common distance measures used include Euclidean distance, Manhattan distance, Mahalanobis distance, or cosine similarity.\n",
    "\n",
    "3. Anomalies are Far from Normal Data: Distance-based methods assume that anomalies are significantly different from normal data points and are located in regions that are far away or dissimilar to the majority of the data. Anomalies are expected to have higher distances or dissimilarities compared to normal data points.\n",
    "\n",
    "4. Global or Local Outliers: Distance-based methods assume that anomalies can be either global outliers or local outliers. Global outliers are data points that are far from the entire dataset, while local outliers are data points that are far from their neighboring data points or clusters.\n",
    "\n",
    "5. Single Cluster or Density Estimation: Some distance-based methods assume that the data contains a single cluster or density peak, and anomalies are data points that are distant from this central cluster. Other methods assume that the data contains multiple clusters or density peaks, and anomalies are data points that do not belong to any cluster or form small, low-density clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores to measure the degree of anomalousness for each data point in a dataset. The anomaly scores provided by LOF reflect the local density deviation of a data point compared to its neighboring data points. Here's how the LOF algorithm computes anomaly scores:\n",
    "\n",
    "1. Nearest Neighbors: For each data point, LOF determines its k nearest neighbors (k is a user-defined parameter). The nearest neighbors are the data points that are closest to the target point based on a chosen distance metric, such as Euclidean distance.\n",
    "\n",
    "2. Reachability Distance: LOF computes the reachability distance for each data point by comparing its distance to its k nearest neighbors. The reachability distance of a data point A to a neighboring data point B is defined as the maximum of the distance between A and B, and the distance between B and its k-th nearest neighbor. The reachability distance captures the local density around a data point.\n",
    "\n",
    "3. Local Reachability Density: The local reachability density (LRD) is computed for each data point by taking the inverse of the average reachability distance of its k nearest neighbors. LRD measures the density of a data point relative to its neighbors. A higher LRD indicates that the data point is in a denser region, while a lower LRD suggests that the data point is in a sparser region.\n",
    "\n",
    "4. Local Outlier Factor: Finally, the local outlier factor (LOF) is calculated for each data point by comparing its LRD to the LRDs of its k nearest neighbors. The LOF of a data point is the average ratio of the LRDs of its neighbors to its own LRD. A LOF value greater than 1 indicates that the data point has a lower density compared to its neighbors, suggesting it is potentially an outlier or anomalous.\n",
    "\n",
    "The anomaly scores provided by LOF reflect the relative degree of anomalousness for each data point. Higher LOF values indicate a higher likelihood of being an anomaly, as the data point is in a region with lower density compared to its neighbors. Conversely, lower LOF values suggest that the data point is in a region with similar density to its neighbors, indicating a more typical or normal instance.\n",
    "\n",
    "By examining the anomaly scores provided by LOF, analysts can identify and rank the data points that are more likely to be anomalies and focus their attention on those instances that deviate significantly from the local density patterns in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is an unsupervised anomaly detection algorithm that isolates anomalies by randomly partitioning the data points. It uses a set of key parameters to control its behavior and effectiveness. Here are the main parameters of the Isolation Forest algorithm:\n",
    "\n",
    "1. n_estimators: This parameter determines the number of isolation trees to be built. An isolation tree is a binary tree that recursively splits the data points based on random attribute and split value selections. Increasing the number of estimators improves the performance of the algorithm but also increases the computational cost.\n",
    "\n",
    "2. max_samples: It determines the maximum number of samples to be used when building each isolation tree. Setting max_samples to a lower value will result in faster computation but may reduce the ability to detect anomalies accurately. The default value is usually set to \"auto,\" which selects a sub-sample of the input data.\n",
    "\n",
    "3. contamination: This parameter specifies the expected proportion of anomalies in the dataset. It helps the algorithm estimate the anomaly threshold. The default value is usually set to \"auto,\" where the algorithm assumes a contamination proportion of 0.1 (10% of the data points are anomalies). Adjusting the contamination value according to the prior knowledge about the dataset can influence the anomaly detection performance.\n",
    "\n",
    "4. max_features: It determines the maximum number of features to consider when splitting the data points in each isolation tree. By default, max_features is set to the square root of the total number of features. Limiting the number of features can help reduce the impact of irrelevant or noisy attributes and improve the algorithm's performance.\n",
    "\n",
    "5. random_state: This parameter is used to initialize the random number generator for reproducibility. Setting a specific random_state value ensures consistent results when running the algorithm multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the anomaly score of a data point using the KNN (K-Nearest Neighbors) algorithm with K=10, we need to consider the local density around the data point and its neighbors. In this scenario, if a data point has only 2 neighbors of the same class within a radius of 0.5, we can compute its anomaly score as follows:\n",
    "\n",
    "1. Compute the K-distance: The K-distance of the data point is the distance to its 10th nearest neighbor. Since the data point has only 2 neighbors within a radius of 0.5, it does not have 10 nearest neighbors. Therefore, the K-distance cannot be computed accurately.\n",
    "\n",
    "2. Compute the Reachability Distance: The reachability distance is computed for each neighbor of the data point relative to the data point itself. Since there are only 2 neighbors, we can calculate the reachability distance for each of them.\n",
    "\n",
    "3. Compute the Local Reachability Density (LRD): The LRD is the inverse of the average reachability distance of the data point's neighbors. However, in this case, there are only 2 neighbors. Therefore, we cannot accurately compute the LRD as we don't have enough neighbors.\n",
    "\n",
    "4. Compute the Local Outlier Factor (LOF): The LOF measures the anomaly score of the data point based on its LRD compared to the LRDs of its neighbors. Since we couldn't accurately compute the LRD, we cannot accurately compute the LOF and, consequently, the anomaly score.\n",
    "\n",
    "In summary, due to the limited number of neighbors and insufficient information to compute the necessary distances and densities, it is not possible to calculate the anomaly score for the data point using the KNN algorithm with K=10 in this particular scenario. An accurate anomaly score calculation typically requires a sufficient number of neighbors and a well-defined neighborhood around the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 1.0056551620519056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "# Generate a synthetic dataset with 3000 data points\n",
    "dataset_size = 3000\n",
    "X, _ = make_blobs(n_samples=dataset_size, centers=1, random_state=42)\n",
    "\n",
    "# Assuming your data point with average path length 5.0\n",
    "data_point = np.array([1, 2])  # Adjust the values accordingly\n",
    "\n",
    "# Create an Isolation Forest instance with 100 trees\n",
    "isolation_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
    "\n",
    "# Fit the model to the dataset\n",
    "isolation_forest.fit(X)\n",
    "\n",
    "# Compute the average path length of the trees\n",
    "average_path_length_trees = 2 * (np.log2(dataset_size - 1) + 0.5772156649)\n",
    "\n",
    "# Compute the anomaly score for the data point\n",
    "average_path_length_data_point = isolation_forest.decision_function([data_point])[0]\n",
    "anomaly_score = 2 ** (-average_path_length_data_point / average_path_length_trees)\n",
    "\n",
    "# Print the anomaly score\n",
    "print(f\"Anomaly Score: {anomaly_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
