{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 APRIL ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning approach in the ensemble method family. It is a strong approach that may be applied to both regression and classification situations. Gradient Boosting Regression seeks to generate a strong predictive model in the setting of regression by integrating numerous weak predictive models, often decision trees.\n",
    "\n",
    "The approach works by adding new decision trees to the ensemble repeatedly while focusing on the residuals (the disparities between predicted and actual values) of prior models. In each iteration, a new decision tree is trained to anticipate the current ensemble's residuals. After then, the forecasts of all the trees in the ensemble are integrated to give the final prediction.\n",
    "\n",
    "Gradient Boosting Regression seeks to optimise a loss function by determining the optimal weights or coefficients for the ensemble's weak models. The optimisation is carried out via gradient descent, which calculates the gradients of the loss function with respect to the predictions. The new weak model is fitted to minimise the loss function in relation to the negative gradients, thereby updating the ensemble in the direction that minimises error.\n",
    "\n",
    "Gradient Boosting Regression has a number of advantages. It can handle a variety of data kinds, including numerical and categorical properties. It is capable of capturing complicated non-linear correlations and variable interactions. It also has built-in feature significance measurements, allowing you to identify the most significant elements in the prediction process.\n",
    "\n",
    "It should be noted, however, that Gradient Boosting Regression is a computationally costly approach that may need careful adjustment of hyperparameters to get optimal performance. To prevent overfitting and increase generalisation, regularisation approaches such as restricting tree depth or utilising learning rate shrinkage are frequently used.\n",
    "\n",
    "Gradient Boosting Regression is a very successful regression approach that provides correct predictions by exploiting the strengths of numerous weak models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 36.00000000564406\n",
      "R-squared: -3.5000000007055077\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert y to float64\n",
    "        y = y.astype(np.float64)\n",
    "\n",
    "        # Initialize the y_hat as the mean of the target variable\n",
    "        y_hat = np.full_like(y, np.mean(y))\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the residual\n",
    "            residual = y - y_hat\n",
    "\n",
    "            # Fit a decision tree regressor to the residual\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "\n",
    "            # Update y_hat by adding the prediction of the tree scaled by the learning rate\n",
    "            y_hat += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Store the model for later prediction\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = np.zeros(X.shape[0])\n",
    "\n",
    "        for tree in self.models:\n",
    "            y_hat += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "# Create a small dataset for demonstration\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Initialize and fit the gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X, y)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred = gb.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Mean Squared Error: 0.004608510740488687\n",
      "R-squared: 0.9999967690220585\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the gradient boosting regressor\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the training data\n",
    "y_pred = best_model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner is a basic and generally low-complexity model that serves as the ensemble's basis model. Weak learners are frequently decision trees, particularly shallow decision trees with few levels or nodes. These decision trees are generally referred to be \"weak\" since they have poor predictive potential and are prone to significant bias on their own.\n",
    "\n",
    "The goal of utilising weak learners in Gradient Boosting is to iteratively combine them to generate a strong prediction model. Each weak learner in the ensemble is trained using the residuals or mistakes of the preceding model. The weak learners try to catch patterns or information that were not properly captured by the prior models by focusing on the residuals.\n",
    "\n",
    "Gradient Boosting adds a new weak learner to the ensemble with each iteration, and its predictions are mixed with the predictions of the previous weak learners. The aggregation of these weak learners eventually improves the ensemble's prediction accuracy, lowering both bias and variation. Gradient Boosting learning involves modifying the weights or coefficients of the weak learners in order to minimise the loss function.\n",
    "\n",
    "There are various advantages of using weak learners in Gradient Boosting. For starters, weak learners are computationally efficient and can be taught fast, making the ensemble's entire training process more efficient. Second, the mixture of weak learners aids in the handling of complicated links and interactions in the data, enhancing the model's overall predictive capacity. Finally, weak learners give some regularisation, which prevents overfitting and improves generalisation.\n",
    "\n",
    "It's worth mentioning that poor learners might differ depending on how Gradient Boosting is used. While decision trees are frequently used as weak learners, alternative algorithms, like as linear models or shallow neural networks, can also be utilised depending on the task and the algorithm's implementation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting approach is designed to develop a strong predictive model repeatedly by integrating numerous weak models in such a manner that each new model concentrates on the mistakes or residuals of the preceding models. Here's a visual breakdown of how Gradient Boosting works:\n",
    "\n",
    "1. Initialization:The method begins by initialising the predictions using a basic model or a constant value. This first forecast is frequently set as the target variable's average or median.\n",
    "\n",
    "2. Calculation of Residuals: The method computes residuals or errors by subtracting the original predictions from the actual target values. These residuals show the amount of information that the initial model did not capture.\n",
    "\n",
    "3. Building Weak Models: To forecast the residuals, a weak model, usually a decision tree with a minimal depth, is trained. The weak model is fitted to the data features and learns to capture the patterns in the residuals.\n",
    "\n",
    "4. Updating Predictions:Predictions are updated by adding the predictions of the weak model to the existing forecasts. The goal of this update is to correct or enhance the predictions using the information supplied by the weak model.\n",
    "\n",
    "5. Iterative Process: Steps 3 and 4 are repeatedly repeated, with each new weak model focused on the residuals or mistakes of the preceding models. The method emphasises situations that are difficult to forecast properly in each iteration.\n",
    "\n",
    "6. Combining Weak Models: Weak Model Combination: The final prediction is derived by combining the predictions of all the weak models in the ensemble. Each weak model makes a weighted prediction, with the weights defined by the algorithm during training.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialization: The algorithm starts by initializing the ensemble with a base model, which can be a simple model or a constant value. This initial prediction is often set as the average or the median of the target variable.\n",
    "\n",
    "2. Residual Calculation: The algorithm calculates the residuals or errors by subtracting the initial predictions from the actual target values. These residuals represent the amount of information that is not captured by the initial model.\n",
    "\n",
    "3. Building a Weak Learner: A weak learner, typically a decision tree with shallow depth, is trained to predict the residuals. The weak learner is fit on the features of the data, considering the residuals as the target variable. It learns to capture the patterns in the residuals.\n",
    "\n",
    "4. Updating Predictions: The predictions of the weak learner are then added to the current predictions of the ensemble. This update aims to correct or improve the predictions by considering the information provided by the weak learner.\n",
    "\n",
    "5. Gradient Calculation: The algorithm calculates the negative gradient of the loss function with respect to the current predictions. The negative gradient provides the direction for updating the predictions in the subsequent iteration. It indicates how the predictions should be adjusted to minimize the loss function.\n",
    "\n",
    "6. Updating the Ensemble: The predictions of the weak learner, scaled by a learning rate, are added to the current predictions of the ensemble. The learning rate controls the contribution of each weak learner, preventing rapid changes and ensuring a smooth convergence of the algorithm.\n",
    "\n",
    "7. Iterative Process: Steps 3 to 6 are repeated iteratively. In each iteration, a new weak learner is trained on the residuals, and its predictions are added to the current predictions of the ensemble. The algorithm continues to focus on the residuals, with each new weak learner targeting the errors that the ensemble has not yet captured.\n",
    "\n",
    "8. Final Prediction: The final prediction is obtained by combining the predictions of all the weak learners in the ensemble. Each weak learner's prediction is weighted according to its learning rate and its contribution to the overall performance.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting method entails comprehending the technique's essential components and phases. The following are the major steps in the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. The procedure begins by establishing an appropriate loss function that assesses the difference between the predicted and true values. The mean squared error (MSE) is a regularly used loss function for regression issues, whereas the log loss or exponential loss functions can be utilised for classification problems.\n",
    "\n",
    "2. The technique begins by initialising the model with a constant value or a basic model, which is frequently specified as the average of the target variable for regression tasks or the logarithmic odds for binary classification jobs. This basic model serves as the foundation for further versions.\n",
    "\n",
    "3. Gradient Calculation: The loss function's gradient with respect to the predictions is computed. For updating the projections, this gradient shows the direction and size of the sharpest rise or decline. The gradient relates to the negative gradient of the loss function in regression problems, and the derivatives of the loss function in classification issues.\n",
    "\n",
    "4. Creating Weak Learners: A weak learner, often a shallow-depth decision tree, is trained to predict the negative gradient or derivatives of the loss function. The weak learner is fitted to the data features, with the negative gradient serving as the goal variable in the regression case and the derivatives acting as the target variable in the classification case. The weak learner learns to recognise patterns in the residuals or information that the prior models did not recognise.\n",
    "\n",
    "5. Predictions are updated by adding the weak learner's predictions to the existing predictions, which are generally scaled by a learning rate or a shrinkage factor. This update step shifts the predictions in the direction indicated by the negative gradient, with the goal of lowering the loss function. The learning rate governs how much each weak learner contributes to the aggregate prediction, limiting abrupt shifts and promoting steady convergence.\n",
    "\n",
    "6. Iterative Process: Steps 3–5 are iteratively repeated. In each cycle, a new weak learner is trained on the loss function's negative gradient or derivatives, and its predictions are added to the existing predictions. Iteratively, the algorithm updates the predictions and reduces the loss function.\n",
    "\n",
    "7. Combining Weak Learners: The final prediction is derived by combining all of the weak learners' predictions in the ensemble. The forecast of each weak learner is weighted based on its learning rate and contribution to total performance. In comparison to individual weak learners, the combination of weak learners results in a better and more accurate prediction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
