{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 APRIL ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours (KNN) is a supervised machine learning technique that is used for classification and regression applications. It is a non-parametric and lazy learning method, which means it makes no assumptions about the underlying data distribution and delays generalisation until new input data is supplied.\n",
    "\n",
    "The \"K\" in KNN stands for the number of nearest neighbours to consider. Given a new, unlabeled input, the algorithm searches the training set for the K closest labelled data points based on a distance metric (such as Euclidean distance) and assigns the majority class (for classification) or calculates the average (for regression) of those K neighbours to determine the label or value for the new data point.\n",
    "\n",
    "Here is a bit by bit outline of the KNN calculation:\n",
    "\n",
    "1. Pick the worth of K (the quantity of neighbors to consider).\n",
    "2. Determine the distance that separates the brand-new, unlabeled input from the entire training set of labeled data points.\n",
    "3. Based on the distances that have been calculated, select the K closest neighbors.\n",
    "4. For classification: Appoint the class name that shows up most often among the K neighbors to the new piece of information.\n",
    "Regression for: Assign the new data point the average of the target values for the K neighbors.\n",
    "5. Yield the anticipated class name or an incentive for the new data of interest.\n",
    "\n",
    "\n",
    "It is essential to keep in mind that the KNN algorithm's performance can be significantly affected by the distance metric and the choice of K. Also, the calculation accepts that the close by information focuses will quite often have comparative names or values. KNN is relatively straightforward and simple to comprehend, but it can be computationally costly, particularly when working with large training datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the value of K in KNN is a crucial aspect that can impact the algorithm's performance. The best value for K is determined by the dataset and the situation at hand. Here are some methods for determining the value of K:\n",
    "\n",
    "1. As a rule of thumb, the square root of the total number of data points in the training set is used as the value of K. If you have 100 data points, for example, you can begin with K=10 (square root of 100).\n",
    "\n",
    "2. Cross-validation: Cross-validation can assist in determining the performance of the KNN algorithm for various K values. You may train and assess the algorithm for different values of K by dividing the training data into distinct subsets (folds). On the validation set, the value of K that produces the greatest performance (e.g., highest accuracy or lowest error) can be chosen.\n",
    "\n",
    "3. Consider the nature of the problem as well as the features of the dataset. Depending on the complexity of the data or the decision limits, other values of K may be more appropriate. A bigger value of K, for example, might assist smooth out the effect of individual noisy data points if the data contains a lot of noise or outliers.\n",
    "\n",
    "4. Grid search: A grid search is performed by testing the KNN algorithm with several K values and selecting the one that produces the highest performance based on a given evaluation parameter (e.g., accuracy, precision, recall). Grid search entails experimenting with a variety of K values and assessing each one using cross-validation or a separate validation set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary distinction between the KNN classifier and KNN regressor is the sort of issue they are employed to solve: classification vs regression. Here's an explanation for each:\n",
    "\n",
    "The KNN Classifier:\n",
    "\n",
    "1. Classification: The KNN classifier is used to perform classification tasks, such as assigning a class label to a given input data point. Examples include determining if an email is spam or not, categorising photographs, and determining the emotion of a document.\n",
    "2. The KNN classifier's output is the class label of the majority of the K closest neighbours to the input data point.\n",
    "3. The classification choice rule entails picking the class label that appears the most frequently among the K neighbours.\n",
    "\n",
    "4. KNN classifier performance is frequently evaluated using measures such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "\n",
    "Regressor KNN:\n",
    "\n",
    "1. Regression: The KNN regressor is used to predict a continuous numerical value for a given input data point in regression problems. Predicting the price of a property based on its features, calculating the temperature based on meteorological conditions, or anticipating the sales volume of a product are some examples.\n",
    "2. The average (or weighted average) of the target values of the K closest neighbours to the input data point is the output of the KNN regressor.\n",
    "\n",
    "3. The regression decision rule includes computing the average (or weighted average) of the goal values of the K neighbours.\n",
    "4. Evaluation: Metrics like as mean squared error (MSE), mean absolute error (MAE), and R-squared are frequently used to analyse the performance of the KNN regressor.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the job, the performance of the KNN algorithm may be assessed using several evaluation measures, such as classification or regression. The following are some standard performance indicators for assessing KNN:\n",
    "\n",
    "For Classification:\n",
    "\n",
    "1. Accuracy is the fraction of correctly categorised data points in comparison to the total amount of data points.\n",
    "\n",
    "2. A confusion matrix is a thorough description of the true positive, true negative, false positive, and false negative predictions. It assesses performance across classes and computes measures like as accuracy, recall, and F1-score.\n",
    "\n",
    "3. Precision is calculated by dividing the number of genuine positive predictions by the total number of projected positive cases. It displays the proportion of projected positive events that are real positives.\n",
    "\n",
    "4. Recall (Sensitivity or True Positive Rate): The ratio of true positive predictions to total positive cases is calculated by recall. It measures how successfully the classifier recognises positive cases.\n",
    "\n",
    "5. F1-score: The harmonic mean of accuracy and recall is the F1-score. When the class distribution is skewed, it gives a single statistic that balances accuracy and recall.\n",
    "\n",
    "\n",
    "For Regression:\n",
    "\n",
    "1. Mean Squared Error (MSE): MSE calculates the average squared difference between anticipated and actual data. It penalises greater mistakes more than smaller ones.\n",
    "\n",
    "2. Mean Absolute Error (MAE): The average absolute difference between projected and actual target values is measured by MAE. Because it is not squared, it gives a more interpretable measure than MSE.\n",
    "\n",
    "3. R-squared (Coefficient of Determination): R-squared reflects the fraction of variation in the target variable that the predicted values can explain. It has a scale of 0 to 1, with 1 indicating a perfect fit.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" refers to the difficulties and constraints that emerge when dealing with high-dimensional data in machine learning, such as the KNN method. It is the phenomena in which the performance of some algorithms degrades as the number of dimensions (features) in the data grows.\n",
    "\n",
    "The following are some significant characteristics of KNN's curse of dimensionality:\n",
    "\n",
    "1. Increased sparsity: As the number of dimensions grows, the accessible data points in high-dimensional space become sparser. In other words, the data gets more dispersed, making it more difficult to locate adjacent neighbours.\n",
    "\n",
    "2. Increased computational complexity: As dimensionality grows, so does the computing cost of calculating distances between data points. The amount of time and memory needed to find nearest neighbours grows exponentially with the number of dimensions.\n",
    "\n",
    "3. Loss of discrimination: The concept of distance becomes less significant in high-dimensional settings. The distance between the nearest and furthest spots tends to converge, resulting in decreased discriminating power. As a result, classifications or forecasts may become less dependable and accurate.\n",
    "\n",
    "4. Overfitting: The danger of overfitting grows with high-dimensional data. The model may become too sensitive to noise or irrelevant information, resulting in poor generalisation to previously unknown data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle missing values in KNN, the missing values must be imputated or filled in before the algorithm is applied. Here's a method for dealing with missing values in KNN:\n",
    "\n",
    "1. find missing values: First, find the features or variables in your dataset that have missing values. This can be accomplished by looking for null values or by employing special markers for missing data (for example, NaN or a specified value).\n",
    "\n",
    "2. Select an imputation method: In KNN, there are numerous ways for filling in missing values. Among the most frequent ways are:\n",
    "\n",
    "a. Mean or median imputation: Replace the missing values with the mean or median value of the feature across the available data points. This is suitable for numerical features.\n",
    "\n",
    "b. Mode imputation: Replace the missing values with the mode (most frequent value) of the feature across the available data points. This is suitable for categorical features.\n",
    "\n",
    "c. Regression imputation: Treat the feature with missing values as the dependent variable and use other features to predict the missing values through regression models. This can be done using linear regression, decision trees, or other regression techniques.\n",
    "\n",
    "d. KNN imputation: Use the KNN algorithm itself to impute missing values. In this approach, the missing values are filled in by averaging or taking the majority value of the nearest neighbors' corresponding feature values. The KNN algorithm is applied separately for each feature with missing values.\n",
    "\n",
    "3. Implement the imputation technique of choice: Fill in the missing values for each feature using the imputation technique you've chosen. This may be accomplished by utilising accessible data preparation packages or programming tools.\n",
    "\n",
    "\n",
    "4. Continue with KNN: After dealing with the missing values, you may use the KNN method as normal. The imputed dataset may be utilised for training, testing, and further analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the KNN classifier and regressor is determined by the task at hand. Here's a comparison of their properties, as well as suggestions on which sort of problem each is most suited for:\n",
    "\n",
    "The KNN Classifier:\n",
    "\n",
    "1. The KNN classifier is well-suited for tackling classification issues in which the aim is to give category labels to input data points.\n",
    "2. Performance metrics: The accuracy, precision, recall, and F1-score of the KNN classifier are commonly used to evaluate its performance.\n",
    "3. The classification choice rule entails picking the class label that appears the most frequently among the K closest neighbours.\n",
    "\n",
    "4. Strengths: Effective for situations with clearly defined classes or decision boundaries.Can handle categorization issues with several classes.The non-parametric character of the method enables for flexibility and adaptability to various data distributions.\n",
    "5. Considerations: Susceptible to irrelevant or distracting aspects.\n",
    "The computational cost rises as the amount of data points or dimensions increases.\n",
    "\n",
    "\n",
    "Regressor KNN:\n",
    "\n",
    "1. The KNN regressor is appropriate for addressing regression issues in which the aim is to predict continuous numerical values for input data points.\n",
    "2. Metrics for evaluating the performance of the KNN regressor include mean squared error (MSE), mean absolute error (MAE), and R-squared.\n",
    "3. The regression decision rule includes computing the average (or weighted average) of the goal values of the K nearest neighbours.\n",
    "4. Advantages: Capable of capturing non-linear correlations between characteristics and target variables.Problems with various input characteristics and complicated interactions are handled.The non-parametric structure of the model offers for greater modelling flexibility.\n",
    "\n",
    "5. Considerations: This model is sensitive to outliers in the training data.\n",
    "The computational cost rises as the amount of data points or dimensions increases.\n",
    "\n",
    "\n",
    "Which one to choose:\n",
    "\n",
    "* For classification problems where the goal is to assign categorical labels, the KNN classifier is generally the appropriate choice. It works well when classes are well-separated, and the decision boundaries are distinct.\n",
    "* For regression problems where the goal is to predict continuous numerical values, the KNN regressor is a suitable option. It is effective in capturing non-linear relationships and can handle complex data patterns.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN method has advantages and disadvantages when used for classification and regression problems. Here are the primary strengths and limitations of the KNN algorithm, as well as potential solutions:\n",
    "\n",
    "KNN's advantages include the following:\n",
    "\n",
    "1. KNN is simple to learn and apply, making it an accessible algorithm for novices.\n",
    "\n",
    "2. Non-parametric: Because KNN makes no assumptions about the underlying data distribution, it is adaptable and appropriate to a wide range of datasets.\n",
    "\n",
    "3. KNN is adaptable enough to tackle multi-class classification issues as well as non-linear relationships in regression applications.\n",
    "\n",
    "KNN's flaws include the following:\n",
    "\n",
    "1. Computational complexity: KNN's biggest shortcoming is its computational cost, which increases as the dataset becomes larger. Distance calculations between data points may be time-consuming, especially in high-dimensional spaces.\n",
    "\n",
    "2. Sensitivity to feature scaling: Because KNN analyses the distance between data points, it is sensitive to feature scale. Larger scale features may dominate the distance computation, affecting the algorithm's performance.\n",
    "\n",
    "3. The curse of dimensionality: As the number of dimensions rises, KNN performance tends to worsen due to increased data sparsity and loss of discriminating between points.\n",
    "\n",
    "\n",
    "Addressing KNN's flaws:\n",
    "\n",
    "1. Efficient data structures: Using efficient data structures such as KD-trees or ball trees helps speed up the search for nearest neighbours, lowering the computing cost of KNN.\n",
    "\n",
    "2. Normalise or scale the characteristics to ensure they have equal ranges. Standardisation or normalisation strategies, such as min-max scaling or z-score normalisation, can assist reduce sensitivity to feature scaling.\n",
    "\n",
    "3. Dimensionality reduction: Use dimensionality reduction techniques like as Principal Component Analysis (PCA) or t-SNE to minimise the number of dimensions and alleviate the curse of dimensionality. This can increase algorithm performance and minimise computational complexity.\n",
    "\n",
    "4. Feature selection: To increase the algorithm's performance and prevent the curse of dimensionality, pick useful characteristics and delete unnecessary or redundant ones.\n",
    "\n",
    "5. Optimal K value: Experiment with various K values and evaluate their influence on the algorithm's performance. Use cross-validation or validation sets to find the best value of K that balances bias and variance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the Euclidean distance and the Manhattan distance are distance metrics that are often employed in the KNN method to calculate the closeness of data points. Here is the distinction between Euclidean and Manhattan distances:\n",
    "\n",
    "Distance in Euclidean terms:\n",
    "\n",
    "1. The straight-line distance between two places in Euclidean space is defined as the Euclidean distance. It is determined as the square root of the total of the squared discrepancies between the two places' coordinates.\n",
    "\n",
    "2. Features: Reflects the actual geometric distance between two places.Considers both the size and direction of coordinate discrepancies.Outliers or excessive values should be avoided.Appropriate for features with a continuous or interval scale.\n",
    "\n",
    "Distance from Manhattan:\n",
    "\n",
    "1. Manhattan distance, also known as city block distance or taxicab distance, is the distance between two places calculated by adding the absolute differences in their coordinates.\n",
    "\n",
    "2. Characteristics: Denotes the distance between two places along orthogonal axes (horizontal and vertical).Ignores magnitude and focuses solely on coordinate discrepancies.When compared to Euclidean distance, it is less susceptible to outliers.Suitable for categorical or ordinal characteristics, as well as circumstances where movement is limited to specified directions (e.g., grid-like structures).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is critical in KNN because it ensures that all features contribute equally to the distance computation between data points. Here's why feature scaling matters in KNN:\n",
    "\n",
    "1. Distance-based algorithm: KNN determines the closeness of data points by computing distances between them. The distance between two data points is calculated using their qualities or properties. If the scales or units of the features differ, certain characteristics with larger scales may dominate the distance computation, impacting the results.\n",
    "\n",
    "2. Equal relevance of features: When computing distances in KNN, all characteristics should ideally have equal value. When a feature has a bigger size, it contributes more to the total distance computation, perhaps overshadowing the effect of other features. This problem is addressed by feature scaling, which brings features to a comparable size.\n",
    "\n",
    "3. Achieving a balanced influence: By scaling the characteristics, you guarantee that each contributes proportionately to the distance calculation. This results in a more balanced depiction of the data and avoids any particular feature from exerting undue influence based only on its magnitude.\n",
    "\n",
    "4. Handling multiple measurement units: It is usual in KNN to have features with several measurement units. For example, one characteristic may indicate age in years and another income in dollars. The ability to scale the characteristics enables for easier comparison and combining of various sorts of metrics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
