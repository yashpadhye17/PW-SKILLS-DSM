{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 APRIL ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major distinction between the Euclidean distance metric and the Manhattan distance metric is how they calculate the distance between two points in a three-dimensional space.\n",
    "\n",
    "The straight-line distance between two places in a Euclidean space is known as the Euclidean distance. In other words, it computes the square root of the sum of squared discrepancies between two locations' corresponding coordinates. In a two-dimensional space, the Euclidean distance between two points (x1, y1) and (x2, y2) is given by:\n",
    "\n",
    "Euclidean distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "\n",
    "The Manhattan distance is the distance between two places measured at right angles along the axes. It computes the total of the absolute differences between two places' corresponding coordinates. The Manhattan distance in a two-dimensional space between two locations (x1, y1) and (x2, y2) is given by:\n",
    "\n",
    "Manhattan distance = |x2 - x1| + |y2 - y1|\n",
    "\n",
    "The distinction between the two metrics is how each coordinate's contribution is considered. The Euclidean distance considers squared differences, which emphasises big discrepancies between locations. Manhattan distance, on the other hand, takes absolute disparities into account, considering all coordinate variances equally.\n",
    "\n",
    "A KNN classifier or regressor's performance can be affected by the decision between Euclidean and Manhattan distance. Here are some things to think about:\n",
    "\n",
    "1. Sensitivity to feature scales: The Euclidean distance is sensitive to feature scale. If the scales of the characteristics differ, the one with the higher value may dominate the distance computation. Feature normalisation or scaling is frequently necessary in such circumstances to balance the contributions. Because Manhattan distance is based on absolute disparities, it is less impacted by varying scales.\n",
    "\n",
    "2. Dimensionality: The curse of dimensionality can make Euclidean distance less effective in high-dimensional areas. As the number of dimensions rises, the distance between points becomes more consistent, making it difficult to discern between neighbours. Manhattan distance, on the other hand, is less influenced by the curse of dimensionality because it only examines variations along each axis.\n",
    "\n",
    "\n",
    "3. Decision boundaries: The form of decision borders in KNN classification can be affected by the distance measure used. Manhattan distance creates choice boundaries that are more aligned with the coordinate axes, generating diamond-like structures, whereas Euclidean distance produces circular decision boundaries. The decision border shape can affect the classifier's ability to properly differentiate distinct classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best value of k, the number of nearest neighbours to include in a KNN classifier or regressor, is a critical decision that can affect the model's performance. Here are a few approaches for determining the best k value:\n",
    "\n",
    "1. Brute-force search: One basic technique is to experiment with different values of k and evaluate the model's performance using an appropriate evaluation measure. You may run a grid search across a range of k values to find the one with the greatest performance. This approach can be computationally demanding, particularly when dealing with huge datasets or high-dimensional areas.\n",
    "\n",
    "2. Cross-validation: Cross-validation is a reliable model evaluation approach. One popular method is k-fold cross-validation, which divides the data into k equal-sized folds. The model is trained and tested k times, each time with a different fold serving as the validation set and the remaining folds serving as the training set. You may discover the k value that delivers the highest performance by averaging the evaluation metrics across all folds for different k values.\n",
    "\n",
    "3. The elbow approach is a graphical tool that can assist in determining the best k value. It entails charting the performance metric (for example, accuracy or error rate) against various values of k. As k grows larger, the model becomes more sophisticated and the training error lowers. However, after attaining an ideal value of k, performance on unknown data may begin to decrease. The elbow point on the curve depicts the k value at which the marginal increase in performance is dramatically reduced, suggesting the best k value.\n",
    "\n",
    "4. Analysing learning curves: Learning curves can reveal information about the model's performance as a function of the size of the training set or the number of neighbours (k). You may monitor the trend and discover the value of k where the model strikes a balance between underfitting and overfitting by graphing the training and validation errors against different values of k. The ideal k value is one that results in the lowest validation error while minimising the difference between training and validation errors.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance measure used in a KNN classifier or regressor can have a substantial impact on its performance. distinct distance measures capture distinct aspects of data point similarity or dissimilarity. Here's how the distance measure you use might affect performance and scenarios when you might choose one over the other:\n",
    "\n",
    "1. Distance in Euclidean terms:\n",
    "\n",
    "* Impact on performance: When the data points are continuous and have similar scales, Euclidean distance works effectively. It calculates the straight-line distance between two places and takes into account the squared differences in coordinates. It is sensitive to difference in feature sizes, and bigger variances in feature scales can dominate the distance computation.\n",
    "\n",
    "* When dealing with continuous numerical features, such as in computer vision tasks or regression issues, Euclidean distance is widely employed. When the feature scales are similar and there is no considerable concern about the curse of dimensionality, it is effective.\n",
    "\n",
    "2. The Manhattan distance (also known as the L1 distance or city block distance) is as follows:\n",
    "\n",
    "* The Manhattan distance computes the sum of absolute differences between coordinates. When compared to Euclidean distance, it is less sensitive to feature scales. When dealing with high-dimensional spaces or sparse data, it works effectively when the variations along each coordinate axis are equally meaningful.\n",
    "* Suitable Situations: In text mining or natural language processing jobs where the input characteristics are word counts or frequency, Manhattan distance is frequently employed. It is also appropriate when working with categorical characteristics or when it is necessary to minimise the impact of outliers.\n",
    "\n",
    "\n",
    "The selection of distance metrics is determined by the nature of the data, the issue domain, and the unique needs. Here are several scenarios in which you would choose one distance measure over another:\n",
    "\n",
    "* When working with continuous numerical characteristics with similar sizes and when the issue area does not entail a high-dimensional space or sparse data, use Euclidean distance.\n",
    "\n",
    "* When dealing with features of diverse scales or when the differences along each coordinate axis are equally meaningful, use Manhattan distance. It is also appropriate for high-dimensional or sparse data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of KNN classifiers and regressors can be modified to increase their performance. Here are some examples of frequent hyperparameters and their effects on the model:\n",
    "\n",
    "1. Number of neighbours (k): This hyperparameter specifies the number of closest neighbours to take into account while formulating predictions. A higher k number can produce smoother decision limits, but it may also increase bias. A smaller number of k, on the other hand, can catch local patterns but may result in a noisy decision border. It is critical to establish the best balance between overfitting and underfitting by adjusting the value of k.\n",
    "\n",
    "2. Distance metric: As described in a previous question, the distance metric used, such as Euclidean distance or Manhattan distance, can have a substantial impact on the model's performance. The suitable distance measure is determined by the data qualities and the situation at hand.\n",
    "\n",
    "3. Weighting scheme: Weighting schemes may be used in KNN models to allocate varying weights to neighbours based on their distance. Inverse distance weighting (IDW), for example, gives closer neighbours more weight. Weighting can assist prioritise near neighbours while reducing the effect of distant neighbours. Experiment with several weighting systems to discover the one that enhances the model's performance.\n",
    "\n",
    "4. Because KNN is based on distance computations, the magnitude of features can have a major influence on the model's performance. If the features have varied sizes, feature scaling techniques such as normalisation or standardisation should be used to guarantee that all features contribute equally to the distance computations.\n",
    "\n",
    "Here are some ways for tuning these hyperparameters and improving model performance:\n",
    "\n",
    "1. Grid search: Run a grid search across a variety of hyperparameter values, such as different k values or distance measures. Use cross-validation to assess the model's performance for each hyperparameter combination. Based on the evaluation criteria, choose the hyperparameter settings that result in the best performance.\n",
    "\n",
    "2. Instead of endlessly looking through all conceivable permutations, select hyperparameter values at random from predetermined ranges. Cross-validate the model and choose the hyperparameter values that result in the best performance. When the hyperparameter space is big, random search may be more efficient than grid search.\n",
    "\n",
    "3. Automated approaches: To find the optimal hyperparameter values, use automated hyperparameter optimisation techniques such as Bayesian optimisation or evolutionary algorithms. To identify optimal solutions, these algorithms intelligently search the hyperparameter space. Implementing these strategies can be aided by libraries such as scikit-optimize or Optuna.\n",
    "\n",
    "4. Analysis and visualisation: Plot learning curves or validation curves to see how the model performs as different hyperparameter values are changed. Analyse the patterns and choose hyperparameter values that are a good compromise between bias and variance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the training set can have a big influence on how well a KNN classifier or regressor performs. Here's how the size of the training set influences the model, as well as various ways for reducing it:\n",
    "\n",
    "\n",
    "* Impact on performance:\n",
    "\n",
    "1. Small training set: A tiny training set may not provide enough instances for the model to effectively capture the underlying patterns and connections in the data. This can result in excessive bias and underfitting. The model may struggle to generalise adequately to previously encountered cases, resulting in poor performance.\n",
    "\n",
    "2. Large training set: As the size of the training set grows, the model gets access to more diverse instances, helping it to capture the underlying patterns more accurately. This can result in less prejudice and better performance. However, utilising a very large training set might present issues with computational complexity and storage needs.\n",
    "\n",
    "* Techniques to optimize training set size:\n",
    "\n",
    "1. Cross-validation: Cross-validation helps you to make better use of available training data by separating it into numerous folds. This approach aids in estimating the model's performance when varied training set sizes are used. Cross-validation with changing training set sizes allows you to see how the model's performance changes with training set size and find an ideal size that balances performance and resource restrictions.\n",
    "\n",
    "2. Learning curves: Learning curves reveal the link between the size of the training set and the model's performance. You may see if the model is still advancing or has reached a plateau by graphing the model's performance metrics (e.g., accuracy, error rate) against different training set sizes. Learning curves can help you decide if more data is needed to improve the model's performance or if the present training set size is adequate.\n",
    "\n",
    "\n",
    "3. Data augmentation: If more training data cannot be obtained, data augmentation techniques can be used to artificially expand the size and variety of the training set. Data augmentation is the process of creating new examples by performing different transformations to existing data instances, such as rotation, translation, or flipping. This strategy can assist enhance model performance by adding more variants and decreasing overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While KNN is a simple and successful method, it does have significant limitations as a classifier or regressor. Here are a few frequent downsides and techniques for dealing with them:\n",
    "\n",
    "1. KNN's computational complexity scales with the size of the training set since it needs calculating distances between the query instance and all training instances. This can be computationally costly for huge datasets, especially during the prediction phase. To combat this, try adopting approximation closest neighbour techniques, such as k-d trees or ball trees, which speed up the search for nearest neighbours while reducing computational load.\n",
    "\n",
    "2. Memory requirements: In order to make predictions, KNN must hold the whole training dataset in memory. As the training set grows in size, so do the memory needs. Using approximate closest neighbour techniques or data structures that allow for fast storing and retrieval of training instances, such as locality-sensitive hashing (LSH) or hashing-based approaches, is one approach to addressing this issue.\n",
    "\n",
    "3. Sensitivity to irrelevant features: When computing distances, KNN considers all characteristics equally important. If the dataset contains irrelevant or noisy characteristics, the model's performance may suffer. Techniques such as feature selection or feature engineering can be used to locate and delete unimportant characteristics, decreasing noise in the data and increasing model performance.\n",
    "\n",
    "4. The curse of dimensionality: Due to the curse of dimensionality, KNN might struggle with high-dimensional data. The distance between points becomes more consistent in high-dimensional areas, making it more difficult to discover meaningful nearest neighbours. To minimise the dimensionality of the data and focus on the most valuable characteristics, feature selection or dimensionality reduction techniques such as principal component analysis (PCA) or t-SNE can be utilised.\n",
    "\n",
    "5. unbalanced data: KNN is susceptible to unbalanced datasets in which one class is much more abundant than others. This can lead to biassed forecasts that favour the majority class. To solve class imbalance and increase model performance, techniques such as oversampling the minority class, undersampling the majority class, or employing specialised algorithms such as SMOTE (Synthetic Minority Over-sampling Technique) can be used.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
