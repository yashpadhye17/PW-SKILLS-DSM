{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 APRIL ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both polynomial functions and kernel functions are utilised in machine learning methods, albeit for different reasons.\n",
    "\n",
    "Polynomial functions are a sort of mathematical function that involves variables multiplied by coefficients and raised to powers. Polynomial functions can be used as basis functions in machine learning to translate input data into a higher-dimensional space. This can be beneficial for tackling issues that cannot be solved linearly, such as classification difficulties.\n",
    "\n",
    "Kernel functions, on the other hand, are a form of similarity function that is used to compare two data points in a high-dimensional feature space. They are frequently employed in support vector machines (SVMs) to translate input data into a higher-dimensional space where it can be segregated more readily. The radial basis function (RBF) kernel, polynomial kernel, and linear kernel are the most often utilised kernel functions in SVMs.\n",
    "\n",
    "While both polynomial functions and kernel functions entail the transformation of input data into a higher-dimensional space, they serve different roles. Polynomial functions directly alter the input data, whereas kernel functions compute similarity between data points in a high-dimensional space.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly = SVC(kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svm_poly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_poly.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter epsilon () in Support Vector Regression (SVR) determines the margin of tolerance for the prediction error. Specifically, epsilon is the gap between the projected values and the epsilon-tube margin. When the value of epsilon in SVR is increased, the epsilon-tube becomes larger, allowing more data points to be included within the margin of tolerance.\n",
    "\n",
    "As the width of the epsilon-tube rises, so does the number of support vectors. This is due to the fact that support vectors are data points that are either on or within the margin of tolerance. When the margin is broader, more data points will fall inside or close to it, necessitating the use of additional support vectors to define the margin.\n",
    "\n",
    "However, the connection between epsilon and the number of support vectors is not necessarily linear and can vary depending on the data complexity and the individual kernel function utilised. A bigger epsilon number can result in a more generic model with more support vectors, but it can also raise the danger of overfitting if the dataset is not large enough to support a more complicated model. As a result, the right value of epsilon must be chosen based on the unique issue and dataset at hand, taking into account parameters such as model complexity, training time, and accuracy.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kernel function: In higher-dimensional feature space, the kernel function controls the geometry of the decision border. Kernel functions that are commonly used include linear, polynomial, radial basis function (RBF), and sigmoid. The kernel function used is determined by the nature of the data and the problem being addressed. A linear kernel, for example, may be useful for a problem with a linear connection between the input and output variables, but an RBF kernel may be more suited for nonlinear interactions. Increasing the complexity of the kernel function can result in a more accurate model, but it also raises the danger of overfitting.\n",
    "\n",
    "2. C parameter: The C parameter governs the trade-off between the model's complexity and the degree to which mistakes are accepted. A higher C number leads in a more complicated model that attempts to fit the data as closely as possible, whereas a lower C value results in a simpler model that allows for more mistakes. A high value of C is often used when there is a limited tolerance for mistakes, but a lower value of C may be used when the data is noisy or a more generic model is required.\n",
    "\n",
    "3. Epsilon parameter: The epsilon parameter specifies the margin of tolerance for the forecast error. Specifically, epsilon is the gap between the projected values and the epsilon-tube margin. A bigger epsilon number produces a wider epsilon-tube, allowing more data points to be included within the margin of tolerance. This can result in a more comprehensive model with more support vectors, but it also increases the danger of overfitting if the dataset is insufficient to support a more sophisticated model.\n",
    "\n",
    "\n",
    "4. Gamma parameter: The smoothness of the decision border is controlled by the gamma value. A smaller gamma value results in a smoother decision border, whereas a larger gamma value results in a more complicated boundary that may overfit the data. A high gamma number is often used when the data is more complicated and requires a more exact fit, whereas a lower gamma value may be used when the data is simpler or a more generic model is required.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9824561403508771\n",
      "Best parameters: {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['breast_cancer_svc.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "params = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "grid_search = GridSearchCV(SVC(), params, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "tuned_svc = SVC(**grid_search.best_params_)\n",
    "tuned_svc.fit(scaler.transform(data.data), data.target)\n",
    "\n",
    "joblib.dump(tuned_svc, 'breast_cancer_svc.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
