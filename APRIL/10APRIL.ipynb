{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 APRIL ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(uses plan) = P(uses plan|smoker) * P(smoker) + P(uses plan|non-smoker) * P(non-smoker)\n",
    "= 0.4 * 0.5 + 0.7 * 0.5\n",
    "= 0.55\n",
    "\n",
    "Plugging in the given values, we get:\n",
    "\n",
    "P(smoker|uses plan) = 0.4 * 0.5 / 0.55\n",
    "= 0.3636 (rounded to four decimal places)\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.3636 or 36.36%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes is most commonly used to binary or boolean data, where each feature represents a yes/no or true/false value. This sort of data is frequently represented as a binary vector, in which each element corresponds to a feature and takes on a value of 0 or 1 depending on whether or not the feature is present.\n",
    "\n",
    "In contrast, Multinomial Naive Bayes is used for discrete count data, where each feature reflects the frequency of a certain word or term in a document. This sort of data is frequently represented as a vector of non-negative integers, with each element corresponding to a phrase and taking on a value representing the frequency of that term in the text.\n",
    "\n",
    "Both techniques operate on the \"naive\" assumption that the characteristics are conditionally independent given the class label, and they employ Bayes' theorem to compute the probability of each class given the observed data. Bernoulli Naive Bayes and Multinomial Naive Bayes, on the other hand, employ distinct likelihood functions to predict the probability of the characteristics given the class due to the diverse sorts of data they are meant to handle. Bernoulli Naive Bayes, for example, models binary data with a Bernoulli distribution, whereas Multinomial Naive Bayes models discrete count data with a multinomial distribution.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes considers that the features are conditionally independent given the class label and models the likelihood of each feature appearing (or not appearing) in a document or observation. As a result, if a feature has a missing value, Bernoulli Naive Bayes thinks the value is absent at random and does not utilise that feature to compute class probabilities.\n",
    "\n",
    "In practise, if a feature has a missing value, it is usually represented as a special value such as NaN or NULL, and the algorithm simply ignores that feature during the training and testing stages. This indicates that whether or not the missing feature exists has no impact on the categorization choice.\n",
    "\n",
    "However, missing data might have an effect on the effectiveness of Bernoulli Naive Bayes if there is a consistent link between the missing values and the class label. Other procedures, such as imputation or data preparation techniques, may be required to manage missing values in such instances.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes may be used to classify data into several categories. In the case of multi-class classification, Gaussian Naive Bayes calculates the likelihood of seeing the characteristics given each class by estimating the mean and variance of each feature for each class. It then use Bayes' theorem to compute the posterior probability of each class given the characteristics, and the class with the highest probability is chosen as the predicted class.\n",
    "\n",
    "In practise, Gaussian Naive Bayes' decision border between classes is a linear combination of the characteristics, making it less flexible than alternative techniques such as decision trees or support vector machines. However, it is straightforward to construct and may be useful in a variety of situations, particularly when the data is regularly distributed and the classes are well segregated.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
