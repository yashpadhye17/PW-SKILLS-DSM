{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 MARCH ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prominent data normalising approach is min-max scaling, which scales the data so that all characteristics have the same range of values. It converts the input to a number between 0 and 1. It is used in data preparation to scale all features such that they have the same influence on the analysis and modelling process.\n",
    "\n",
    "The min-max scaling approach can be used in the following ways:\n",
    "\n",
    "1. Determine the feature's lowest and maximum values.\n",
    "2. Remove the feature's minimal value from each value.\n",
    "3. Subtract each number from the range (maximum value minus minimum value).\n",
    "The min-max scaling formula is as follows:\n",
    "\n",
    "(max(x) - min(x)) / (x - min(x))\n",
    "\n",
    "where x is the feature value, min(x) is the feature's minimum value, and max(x) is the feature's highest value.\n",
    "\n",
    "\n",
    "\n",
    "For example, consider a dataset that contains a feature \"age\" with a minimum value of 18 and a maximum value of 65. To apply min-max scaling to this feature, we can use the following formula:\n",
    "\n",
    "(age - 18) / (65 - 18)\n",
    "\n",
    "Suppose the age of a person in the dataset is 40. Using the formula, we can calculate the scaled value as follows:\n",
    "\n",
    "(40 - 18) / (65 - 18) = 0.4\n",
    "\n",
    "Therefore, the scaled value of age for this person is 0.4. Similarly, we can apply min-max scaling to all the features in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another typical data normalising approach used in feature scaling is the unit vector technique. It scales the data so that each feature vector becomes a vector of length one, commonly known as a unit vector. It is also known as L2 normalisation.\n",
    "\n",
    "The unit vector approach can be used in the following ways:\n",
    "\n",
    "1. Determine the feature vector's L2 norm, which is the square root of the sum of all the values in the vector.\n",
    "2. Divide each feature vector value by the L2 norm.\n",
    "The unit vector scaling formula is:\n",
    "\n",
    "x / ||x||\n",
    "\n",
    "where x is the feature vector and ||x|| is the feature vector's L2 norm.\n",
    "\n",
    "\n",
    "\n",
    "For example, consider a dataset that contains a feature vector \"height\" and \"weight\" with values of 175 cm and 70 kg, respectively. To apply unit vector scaling to this feature vector, we can use the following formula:\n",
    "\n",
    "[175, 70] / sqrt((175^2 + 70^2))\n",
    "\n",
    "The squared values are summed and then the square root is taken to get the L2 norm. The resulting L2 norm is 185.54.\n",
    "\n",
    "Using the formula, we can calculate the scaled vector as follows:\n",
    "\n",
    "[175, 70] / 185.54 = [0.94299, 0.33282]\n",
    "\n",
    "Therefore, the unit vector for this feature vector is [0.94299, 0.33282]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a prominent dimensionality reduction statistical approach. It is a strategy for converting a high-dimensional dataset of variables into a lower-dimensional space while maintaining as much information as feasible.\n",
    "\n",
    "PCA works by determining the dataset's principle components, which are the directions in which the data fluctuates the most. The original data is then projected onto these primary components to generate a lower-dimensional representation of the data.\n",
    "\n",
    "The following are the steps for using PCA:\n",
    "\n",
    "1. Standardize the data so that the mean of each characteristic is 0 and the standard deviation is 1.\n",
    "2. Calculate the standardised data's covariance matrix.\n",
    "3. Determine the covariance matrix's eigenvectors and eigenvalues.\n",
    "4. Sort the eigenvectors in decreasing order by their associated eigenvalues.\n",
    "5. Choose the top k eigenvectors, where k is the number of dimensions requested for the reduced dataset.\n",
    "6. To obtain the lower-dimensional representation, project the original data onto the k eigenvectors.\n",
    "\n",
    "\n",
    "Consider a dataset with ten features, each representing a distinct measurement collected from a patient, to demonstrate how PCA may be used for dimensionality reduction. We want to save as much information as feasible while reducing the complexity of this dataset to three dimensions.\n",
    "\n",
    "Then, we normalise the data by removing the mean and dividing by each feature's standard deviation. The covariance matrix is then computed, as are the eigenvectors and eigenvalues of the covariance matrix. We sort the eigenvectors by their eigenvalues and choose the top three eigenvectors, which correspond to the top three dimensions with the largest variance. Lastly, to generate the lower-dimensional representation, we project the original data onto these three eigenvectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<U>Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to its more generally recognised usage for dimensionality reduction, PCA may be used for feature extraction. The aim behind utilising PCA for feature extraction is to transform the dataset's original features into a new collection of features that capture the most variance of the data. These new characteristics are called primary components because they are linear combinations of the original elements.\n",
    "\n",
    "Using PCA for feature extraction is analogous to using it for dimensionality reduction:\n",
    "\n",
    "Standardize the data so that the mean of each characteristic is 0 and the standard deviation is 1.\n",
    "1. Calculate the standardised data's covariance matrix.\n",
    "2. Determine the covariance matrix's eigenvectors and eigenvalues.\n",
    "3. Sort the eigenvectors in decreasing order by their associated eigenvalues.\n",
    "4. Choose the top k eigenvectors, where k is the number of new features to be extracted.\n",
    "5. To acquire the new feature set, project the original data onto the k eigenvectors.\n",
    "\n",
    "Consider a dataset with ten features representing various qualities of an automobile to demonstrate how PCA may be used for feature extraction. We want to extract the most relevant variables from this information so that we can develop a model that forecasts the car's fuel efficiency.\n",
    "\n",
    "To begin, we standardise the data, compute the covariance matrix, and compute the eigenvectors and eigenvalues. We sort the eigenvectors by their eigenvalues and choose the top three eigenvectors, which correspond to the three most essential traits. The original data is then projected onto these three eigenvectors to get the new feature set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling can be used to preprocess data before training a machine learning model in the context of developing a recommendation system for a food delivery service. Min-Max scaling is a technique for scaling a feature's values to a specified range between 0 and 1.\n",
    "\n",
    "In this instance, we would apply Min-Max scaling as follows:\n",
    "\n",
    "Determine which characteristics in the dataset should be scaled. We have features such as pricing, rating, and delivery time in this scenario.\n",
    "\n",
    "Determine the lowest and maximum values for each characteristic. For instance, the lowest and maximum price values may be $2 and $30, respectively.\n",
    "\n",
    "Apply the Min-Max scaling algorithm to each feature value:\n",
    "\n",
    "(value - min value) / (max value - min value) = scaled value\n",
    "\n",
    "For example, if a food item costs $10, the scaled value would be:\n",
    "\n",
    "scaled value = (10-2) / (30-2) = 0.28\n",
    "\n",
    "Step 3 should be repeated for each value in the feature.\n",
    "\n",
    "Once all features have been scaled, the data can be used to train a machine learning model for the recommendation system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA may be used to reduce the dimensionality of a dataset when creating a model to forecast stock prices by finding the most essential elements that contribute to the variation in the data.\n",
    "\n",
    "Here's a rundown of the stages needed in applying PCA to reduce dimensionality:\n",
    "\n",
    "1. Normalize the data: Since PCA assumes that the data has been standardised, the first step is to normalise the dataset by scaling each feature to have a mean of zero and a standard deviation of one.\n",
    "\n",
    "2. The covariance matrix represents the connection between each pair of characteristics in the dataset. It is calculated by calculating the dot product of the transposed and original datasets.\n",
    "\n",
    "3. Calculate the covariance matrix's eigenvectors and eigenvalues: The eigenvectors indicate the dataset's main components, while the eigenvalues represent the amount of variation explained by each component.\n",
    "\n",
    "4. Sort the eigenvectors according to their eigenvalues: The eigenvectors with the greatest eigenvalues contribute the most to the dataset's variance.\n",
    "\n",
    "5. To decrease the dimensionality of the dataset to k dimensions, choose the top k eigenvectors with the greatest eigenvalues.\n",
    "\n",
    "6. Use the dot product of the transposed dataset and the selected eigenvectors to translate the data into the new k-dimensional space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform the values to a range of -1 to 1, we need to apply the following formula:\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min) * 2 - 1\n",
    "\n",
    "where X is the original value, X_min is the minimum value in the dataset, and X_max is the maximum value in the dataset.\n",
    "\n",
    "For the given dataset [1, 5, 10, 15, 20], the minimum value is 1 and the maximum value is 20. Therefore, we can perform Min-Max scaling as follows:\n",
    "\n",
    "For the first value:\n",
    "X_scaled = (1 - 1) / (20 - 1) * 2 - 1 = -0.9\n",
    "\n",
    "For the second value:\n",
    "X_scaled = (5 - 1) / (20 - 1) * 2 - 1 = -0.4\n",
    "\n",
    "For the third value:\n",
    "X_scaled = (10 - 1) / (20 - 1) * 2 - 1 = 0.2\n",
    "\n",
    "For the fourth value:\n",
    "X_scaled = (15 - 1) / (20 - 1) * 2 - 1 = 0.6\n",
    "\n",
    "For the fifth value:\n",
    "X_scaled = (20 - 1) / (20 - 1) * 2 - 1 = 1.0\n",
    "\n",
    "Therefore, the Min-Max scaled dataset with values transformed to a range of -1 to 1 is [-0.9, -0.4, 0.2, 0.6, 1.0]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract features using PCA, we must first normalise the features to have a zero mean and unit variance. Following that, we may use PCA to extract the main components.\n",
    "\n",
    "The number of principle components to keep is determined by how much variation we want to keep in the data. Normally, we select a number of primary components that represent a considerable portion of the variation, such as 95% or more.\n",
    "\n",
    "Given that the features have been normalised, we may apply PCA on the dataset. PCA produces a collection of main components, each of which represents a linear combination of the original features. The principle components are arranged in order of the amount of variation explained, with the first principal component explaining the greatest amount of variance.\n",
    "\n",
    "To select how many principle components to keep, we may assess the variation explained by each principal component and establish how many are required to capture a suitable amount of variance. To see this, plot the cumulative explained variance as a function of the number of principle components and select the number of principal components at which the curve begins to level out.\n",
    "\n",
    "\n",
    "In this situation, the number of retained principle components would be determined by the amount of variation explained by each principal component. Without doing the analysis, it is impossible to predict how many principal components to maintain, but we may assume that the first few principle components will capture the majority of the variation, and the latter ones will capture less and less variance.\n",
    "\n",
    "For example, if the first two main components account for 80% of the variation in the data, we may keep those two and eliminate the rest. The precise amount of components to maintain, however, would be determined by the individual dataset and the analysis's aims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
