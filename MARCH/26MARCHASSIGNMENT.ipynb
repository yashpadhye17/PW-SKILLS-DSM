{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26 MARCH ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.</U>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical approach called simple linear regression is used to predict the connection between a dependent variable and a single independent variable. To establish the linear relationship between the two variables, a straight line is fitted to the data points. Simple linear regression is defined as y = 0 + 1x +, where y is the dependent variable, x is the independent variable, 0 is the intercept, 1 is the slope of the line, and is the error term.\n",
    "\n",
    "For example, suppose we wish to investigate the association between student study hours and exam scores. We may model this association using basic linear regression, with the number of hours studied as the independent variable and exam results as the dependent variable.\n",
    "\n",
    "\n",
    "Multiple linear regression, on the other hand, is a statistical approach for modelling the connection between two or more independent variables and a dependent variable. To establish the link between the dependent variable and the independent variables, a linear equation is fitted to the data points with many independent variables. Multiple linear regression equation is y = 0 + 1x1 + 2x2 +... + nxn +, where y is the dependent variable, x1, x2,..., xn are the independent variables, 0 is the intercept, 1, 2,..., n are the coefficients, and is the error term.\n",
    "\n",
    "Assume we wish to investigate the association between a person's income and their amount of education, job experience, and age. We can model this association using multiple linear regression with income as the dependent variable and education level, job experience, and age as the independent variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a statistical approach that generates accurate predictions by making certain assumptions about the data. The major assumptions of linear regression are as follows:\n",
    "\n",
    "* Linearity:Linearity refers to the connection between the independent and dependent variables. This signifies that the dependent variable's change is proportionate to the independent variable's change.\n",
    "\n",
    "* Independence: The observations are distinct from one another. This signifies that the value of the dependent variable for one observation is independent of the value of the dependent variable for another.\n",
    "\n",
    "* Homoscedasticity occurs when the variance of the residuals (the difference between the actual and predicted values of the dependent variable) remains constant across all independent variable values.\n",
    "\n",
    "* Normality: The residuals are distributed regularly around the mean value of zero. This implies that the bulk of residuals should be near zero, with fewer residuals occurring at higher or lower values.\n",
    "\n",
    "* There is no multicollinearity: No perfect linear connection exists between the independent variables. This means that each independent variable should be independent of the others and not perfectly connected with any of them.\n",
    "\n",
    "\n",
    "Many diagnostic charts and tests may be used to determine whether these assumptions hold true in a particular dataset:\n",
    "\n",
    "* A residual plot may be used to visually check if the residuals are randomly distributed around the horizontal line at zero, suggesting homoscedasticity. Heteroscedasticity may exist if the residuals have a cone shape or a pattern.\n",
    "\n",
    "* Normal probability plot: A normal probability plot is a visual representation of the normality assumption. The normalcy assumption is satisfied if the residuals are near to a straight line.\n",
    "\n",
    "* Cook's distance: Cook's distance is a measure of an observation's effect on the regression model. High Cook's distance observations are regarded important and may suggest a breach of the independence assumption."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope and intercept in a linear regression model describe the connection between the independent and dependent variables. The intercept shows the value of the dependent variable when the independent variable is zero, and the slope indicates the change in the dependent variable when the independent variable is increased by one unit.\n",
    "\n",
    "If we wish to use a linear regression model to estimate a person's weight based on their height. This model's equation might be:\n",
    "\n",
    "50 + 0.6 * height = weight\n",
    "\n",
    "The intercept in this equation is 50, which reflects the weight of a human with a height of zero. Yet, because height can never be 0, the intercept has no practical application.\n",
    "\n",
    "The slope is 0.6, which implies that for every unit increase in height, the person's weight rises by 0.6 pound. Hence, if a person's height is 60 inches, we may estimate their weight as follows:\n",
    "\n",
    "weight = 50 + 0.6 * 60 = 86 lbs.\n",
    "\n",
    "As a result, for every additional inch in height, we may expect a 0.6-pound weight gain. Nevertheless, it is crucial to remember that this is only a forecast based on the linear relationship between height and weight in the current dataset, and other factors that impact a person's weight may exist."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q4. Explain the concept of gradient descent. How is it used in machine learning?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gradient descent is a mathematical optimisation approach for determining the minimum value of a function. It is frequently used in machine learning to adjust model parameters in order to minimise the difference between anticipated and real values.\n",
    "\n",
    "The essential notion behind gradient descent is to iteratively modify the parameters in the direction of the cost function's steepest descent (i.e., negative gradient). The cost function measures how well the model matches the training data, and the objective is to decrease it to create the best model feasible.\n",
    "\n",
    "The algorithm computes the gradient (i.e., partial derivative) of the cost function with respect to each parameter in each iteration of gradient descent. The parameters are then updated by subtracting a fraction of the gradient from the current values, which is known as the learning rate. This method is repeated until the cost function approaches a minimum or a predetermined stopping condition is fulfilled.\n",
    "\n",
    "Gradient descent is classified into three types: batch, stochastic, and mini-batch. \n",
    "* Batch gradient descent uses the gradients of the whole dataset to update the parameters, which can be computationally demanding for big datasets. \n",
    "* The parameters for each each observation are updated through stochastic gradient descent, which can result in faster convergence but also increased instability.\n",
    "* Mini-batch gradient descent is a hybrid of the two, with gradients calculated for a fraction of the data in each iteration.\n",
    "\n",
    "Gradient descent is a strong and extensively used machine learning optimisation technique. It's used to fine-tune the parameters of various models, including as linear regression, logistic regression, neural networks, and support vector machines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical approach used to represent the connection between a dependent variable and numerous independent variables is multiple linear regression. It is an extension of simple linear regression in which just one independent variable and one dependent variable are modelled.\n",
    "\n",
    "The multiple linear regression model can be represented as:\n",
    "\n",
    "y = β0 + β1x1 + β2x2 + … + βpxp + ε\n",
    "\n",
    "where y is the dependent variable, x1, x2, ..., xp are the independent variables, β0 is the intercept, β1, β2, ..., βp are the coefficients (or slopes), and ε is the error term.\n",
    "\n",
    "The multiple linear regression model predicts the value of the dependent variable for every combination of the independent variables by estimating the values of the coefficients that best match the data.\n",
    "\n",
    "The number of independent variables used to describe the connection with the dependent variable is the primary distinction between basic linear regression and multiple linear regression. A single independent variable is used in simple linear regression, but two or more independent variables are used in multiple linear regression.\n",
    "\n",
    "The fundamental benefit of multiple linear regression is that it allows us to concurrently account for the effects of numerous independent factors on the dependent variable. This is important when numerous factors are impacting the dependent variable and we want to understand how they all interact.\n",
    "\n",
    "The interpretation of the coefficients in multiple linear regression, on the other hand, is more sophisticated than in basic linear regression. Holding all other independent variables fixed, each coefficient indicates the change in the dependent variable associated with a one-unit increase in the related independent variable. When evaluating the coefficients, it is critical to evaluate the combined impact of all the independent factors on the dependent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is a significant correlation between two or more independent variables, multicollinearity is a typical problem in multiple linear regression. This makes determining the influence of each independent variable on the dependent variable challenging since their effects become muddled.\n",
    "\n",
    "The coefficients of the independent variables may become unstable and their signs may become opposite to what we would predict in the presence of multicollinearity. This might make it tough to comprehend the model and make correct predictions.\n",
    "\n",
    "In multiple linear regression, there are numerous methods for detecting multicollinearity. One popular approach is to construct the correlation matrix of the independent variables and search for high correlations (i.e., values close to 1 or -1). Another approach is to compute the variance inflation factor (VIF), which quantifies how much the variance of the predicted coefficient is inflated as a result of multicollinearity. A VIF larger than 5 or 10 is thought to indicate a significant level of multicollinearity.\n",
    "\n",
    "There are numerous ways that may be used to handle multicollinearity:\n",
    "\n",
    "* Drop one of the correlated independent variables: This is the most straightforward option and can be successful if the independent variables are significantly associated.\n",
    "* Employ principle component analysis (PCA): PCA is a technique for decreasing the dimensionality of independent variables by generating new variables that are linear combinations of the original variables. These additional variables are orthogonal to one another and can aid in the reduction of multicollinearity.\n",
    "\n",
    "* Regularization techniques, such as Lasso or Ridge regression, penalise the size of the coefficients and can aid in the reduction of multicollinearity by decreasing the coefficients towards zero.\n",
    "\n",
    "* Obtain additional data: If possible, increasing the variance in the independent variables can help lessen the influence of multicollinearity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<U>Q7. Describe the polynomial regression model. How is it different from linear regression?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression is a sort of regression analysis in which the connection between the independent variable x and the dependent variable y is represented as a polynomial of nth degree. This is in contrast to linear regression, which uses a straight line to describe the connection between x and y.\n",
    "\n",
    "The polynomial regression model can be represented as:\n",
    "\n",
    "y = β0 + β1x + β2x^2 + … + βnx^n + ε\n",
    "\n",
    "where y is the dependent variable, x is the independent variable, β0, β1, β2, ..., βn are the coefficients, ε is the error term, and n is the degree of the polynomial.\n",
    "\n",
    "The functional form of the model distinguishes polynomial regression from linear regression. The link between x and y is described as a straight line in linear regression, but as a curve of degree n in polynomial regression. This enables polynomial regression to capture nonlinear connections between x and y that linear regression cannot.\n",
    "\n",
    "Polynomial regression can be beneficial in situations when the connection between x and y is curved or nonlinear. For example, if we model the link between a car's age and price, we may discover that the relationship is not linear, but rather follows a curve, with older autos degrading at a faster rate.\n",
    "\n",
    "\n",
    "Polynomial regression, on the other hand, is prone to overfitting, especially when the degree of the polynomial is large. As a consequence, a model that fits the training data well but does not generalise well to new data may be produced. To guarantee that the model is accurate and resilient, it is critical to pick an appropriate degree of the polynomial and to undertake model selection and validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression has the following advantages over linear regression:\n",
    "\n",
    "* Ability to model nonlinear connections: Unlike linear regression, polynomial regression can capture nonlinear relationships between independent and dependent variables.\n",
    "\n",
    "* Polynomial regression can offer a better fit to the data than linear regression in circumstances when the connection between the independent and dependent variables is nonlinear.\n",
    "\n",
    "* Polynomial regression is versatile in that it may be used to model a broad range of connections, from simple curves to complicated, high-degree polynomials.\n",
    "\n",
    "Polynomial regression has the following disadvantages over linear regression:\n",
    "\n",
    "* Overfitting: Polynomial regression models with high degree polynomials are prone to overfitting, resulting in poor generalisation performance on fresh data.\n",
    "\n",
    "* Increasing complexity: As the degree of the polynomial grows, so does the model's complexity. This can make interpreting the model and explaining the link between the independent and dependent variables more difficult.\n",
    "\n",
    "* Increasing computational complexity: As the degree of the polynomial grows, so does the computational complexity of the model, making training and evaluating it more time-consuming.\n",
    "\n",
    "\n",
    "Polynomial regression is often effective when the connection between the independent and dependent variables is nonlinear and linear regression cannot capture the underlying relationship. Polynomial regression may also be beneficial when there are several causes of variation in the data and we want to describe the connection between the independent and dependent variables while taking these sources of variation into consideration.\n",
    "\n",
    "Nevertheless, when employing polynomial regression, especially with high degree polynomials, it is vital to be cautious since they can be prone to overfitting and may not adapt well to fresh data. In these cases, approaches such as regularisation or model selection may be required to guarantee that the model is correct and resilient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
