{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18 MARCH ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<U>Q1. What is the Filter method in feature selection, and how does it work?</U>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter method is a feature selection strategy that uses statistical measurements to identify the most relevant characteristics from a dataset. It operates by assessing the features independently of the machine learning model, implying that it is a preprocessing step performed before to training the model.\n",
    "\n",
    "The filter approach applies a statistical measure to each feature in the dataset and ranks the features based on their scores. The features with the greatest ratings are regarded as the most significant, while those with the lowest values are seen as the least important. Depending on the type of data and the task at hand, the statistical measure used to rank the characteristics may differ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q2. How does the Wrapper method differ from the Filter method in feature selection?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wrapper approach and the Filter method are two popular feature selection strategies. While both strategies strive to choose a subset of traits that are most relevant to the prediction job, they accomplish this objective in different ways.\n",
    "\n",
    "The Filter method is a straightforward and computationally efficient strategy for sorting features based on some statistical metric such as correlation, mutual information, or variance. The top-ranked characteristics are then chosen for inclusion in the model. This strategy does not need model training and is unaffected by the learning algorithm used. It may be used before training a model as a preprocessing step to minimise the amount of features and enhance model performance.\n",
    "\n",
    "The Wrapper technique, on the other hand, evaluates the performance of several feature subsets using a specific machine learning algorithm. It entails picking a subset of features and training a model with that subset of characteristics. The model's performance is then tested using a validation set or cross-validation, and the procedure is repeated for other feature subsets. The final collection of features is chosen from the subset that produces the greatest model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q3. What are some common techniques used in Embedded feature selection methods?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the strategies commonly utilised in embedded feature selection methods are:\n",
    "\n",
    "Lasso regression is a linear regression model that includes a penalty component in the cost function. This penalty term promotes the model to choose just the most relevant attributes while punishing those that aren't. This method can produce sparse solutions with just a fraction of the features having non-zero coefficients.\n",
    "\n",
    "Ridge regression is similar to Lasso regression but has a different penalty term. Ridge regression applies a penalty term to the sum of squared coefficients of the features, shrinking them towards zero but without setting them to zero. This method can result in models that use all of the characteristics but have tiny coefficients for less important ones.\n",
    "\n",
    "Decision trees may also be used to pick embedded features by selecting the most informative characteristics at each node of the tree. Decision trees partition data recursively depending on the most informative feature until a stopping requirement is reached, resulting in a tree structure with the top features being the most significant.\n",
    "\n",
    "Random forests are an ensemble of decision trees, with each tree trained on a different subset of the characteristics. By assessing the decrease in impurity when a feature is used to split the data, random forests may offer an estimate of the relevance of each feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q4. What are some drawbacks of using the Filter method for feature selection?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring inter-feature correlations: The Filter technique ignores inter-feature correlations and only ranks features based on individual statistical measurements. This might result in the selection of redundant characteristics that are highly associated with one another and contribute little value to the model.\n",
    "\n",
    "Lack of flexibility: Because the Filter technique is independent of the learning algorithm used to create the model, it cannot adjust to the model's particular requirements. This may result in the selection of characteristics that are not optimal for the chosen model, lowering model performance.\n",
    "\n",
    "The Filter approach depends on linear statistical measures like as correlation and mutual information, which may not capture nonlinear interactions between data and the target variable. As a result, traits that are not genuinely relevant to the prediction job may be chosen.\n",
    "\n",
    "Overfitting: If the feature selection is conducted on the same dataset that was used to train the model, the Filter approach may result in overfitting. As a result, characteristics that are particular to the training set and do not generalise well to fresh data may be chosen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some circumstances when the Filter technique may be preferable than the Wrapper method:\n",
    "\n",
    "Big datasets: The Wrapper technique may be computationally costly, particularly for large datasets with numerous characteristics. In such instances, the Filter approach, which is often quicker and can handle massive volumes of data, may be more practical and efficient.\n",
    "\n",
    "No access to the target model: To evaluate the performance of different feature subsets, the Wrapper technique requires access to the target model. If you don't have access to the target model or if training the model is challenging, the Filter approach may be a better choice.\n",
    "\n",
    "If the connection between the characteristics and the target variable is linear, the Filter technique may be adequate to choose the most relevant features. Simple statistical measures like correlation and variance, which are widely employed in the Filter approach, may capture linear connections.\n",
    "\n",
    "If you are confident that there are no strongly linked or redundant characteristics in your dataset, the Filter technique may be a viable alternative. In such instances, the Filter approach can effectively choose the most relevant characteristics without requiring additional processing complexity or model training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use the Filter technique to choose the most relevant attributes for the prediction model for customer churn:\n",
    "\n",
    "1. Specify the target variable: The target variable in this example is customer churn. Create a binary variable for churn and define it clearly (1 for churn and 0 for non-churn).\n",
    "\n",
    "2. Choose a statistical measure: The Filter technique can employ a variety of statistical measures, including correlation, mutual information, and the chi-square test. Choose a measure that is appropriate for the data you have and the connections you wish to capture. Correlation, for example, is beneficial for continuous data, but chi-square is better suited for categorical ones.\n",
    "\n",
    "3. Compute the measure for each attribute: Using the selected measure, compute the statistical measure for each attribute in the dataset. This will provide you with a score for each characteristic indicating how strongly it is related to the target variable.\n",
    "\n",
    "4. Rank the attributes: Using the scores determined in step 3, sort the attributes in decreasing order. The qualities with the highest rankings are the ones that are most closely connected with the target variable and should be evaluated for inclusion in the model.\n",
    "\n",
    "5. Examine for feature redundancy: If numerous features are significantly connected with one another, consider deleting one of them to eliminate redundancy.\n",
    "\n",
    "6. Assess the selected qualities: Once you've determined which attributes are the most important, test their performance in a prediction model. To evaluate the model's performance, use an appropriate assessment metric, such as accuracy, precision, recall, or F1 score. Steps 2â€“5 may need to be repeated to fine-tune the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these steps to utilise the Embedded technique to choose the most relevant characteristics for a soccer match prediction model:\n",
    "\n",
    "1. Choose an appropriate algorithm: Model-based feature selection Methods that incorporate the feature selection process into the model training process are known as embedded methods. As a result, pick an approach that enables embedded feature selection, such as LASSO, Ridge regression, Elastic Net, or Decision Tree.\n",
    "\n",
    "2. Prepare the data: Clean, preprocess, and convert the data as needed to prepare the dataset. Handling missing values, normalising or standardising the data, and encoding categorical variables are all examples of this.\n",
    "\n",
    "3. Form the model: Use all available characteristics to train the selected algorithm on the provided dataset. This results in a model that is optimised for prediction accuracy but may contain unnecessary or redundant information.\n",
    "\n",
    "4. Assess feature significance: Using the trained model, assess the relevance of each feature. This may be accomplished by studying the algorithm's coefficients or feature importances. The model considers features with higher coefficients or importances to be more relevant.\n",
    "\n",
    "5. Choose the most important features: Choose the characteristics with the highest priority scores. A cutoff criterion can be used to choose a specified number of features or a percentage of the total number of features.\n",
    "\n",
    "6. Recondition the model: Retrain the algorithm, but this time using only the features you've chosen. This results in a new model that is optimised for prediction accuracy and employs just the most important characteristics.\n",
    "\n",
    "7. Examine the model: Lastly, assess the new model's performance using relevant measures such as accuracy, precision, recall, or F1 score. Steps 4-6 may need to be repeated to fine-tune the model and select the ideal amount of features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these steps to utilise the Wrapper technique to choose the optimal collection of characteristics for estimating the price of a house:\n",
    "\n",
    "1. Define the search space by doing the following: Begin by determining the potential feature subset search space. Create a list of all potential combinations of the available features to do this. For example, if you have three attributes (size, location, and age), you will have eight different subsets:, size, location, age, size, location, size, age, size, age, size, age, location, age, and size, location, age.\n",
    "\n",
    "2. Choose an appropriate algorithm: Choose a good algorithm for training and evaluating the model. Any regression algorithm, such as Linear Regression, Decision Tree Regression, or Random Forest Regression, can be used.\n",
    "\n",
    "3. Form the model: Using the current feature subset, train the selected algorithm on the complete dataset. Use a suitable statistic, such as mean squared error or root mean squared error, to assess the model's performance.\n",
    "\n",
    "4. Assess feature subset performance: Based on the model's performance, evaluate the performance of the current feature subset. This will assist you in determining which feature subsets are most likely to be effective in forecasting property prices.\n",
    "\n",
    "5. Generate new feature subsets: Create new feature subsets by adding or deleting one feature from the existing subset at a time. If the current subset is size, location, age, the new subsets will be size, location, size, age, and location, age.\n",
    "\n",
    "6. Steps 3-5 should be repeated for each additional feature subset until all viable subsets have been analysed.\n",
    "\n",
    "7. Choose the best subset: Select the subset with the best performance depending on the evaluation metric applied. This subgroup will be the most accurate in estimating the price of a house.\n",
    "\n",
    "8. Train the final model as follows: Use the optimal subset of features to train the selected algorithm on the complete dataset. This results in the final model, which can be used to forecast the price of a property based on its qualities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
