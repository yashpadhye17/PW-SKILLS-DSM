{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 MARCH ASSIGNMENT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting happens when a model is overly complicated and closely matches the training data. As a result, the model performs well on training data but badly on fresh, unknown data. Overfitting is an issue because the model memorises the training data rather than learning to generalise from it.\n",
    "\n",
    "Underfitting happens when a model is overly basic and fails to capture the underlying patterns in the data. As a result, the model performs badly on both training and fresh, previously unknown data.\n",
    "\n",
    "Overfitting and underfitting have distinct repercussions. Overfitting can cause poor performance on new data, whereas underfitting can cause poor performance on both training and new data. The model does not generalise effectively from the training data in both scenarios.\n",
    "\n",
    "There are numerous methods for reducing overfitting and underfitting. To avoid overfitting, approaches such as regularisation, which penalises too complicated models, might be used. Cross-validation may also be used to assess the model's performance on fresh, previously unknown data. To reduce underfitting, use a more sophisticated model, acquire more data, or use better features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q2: How can we reduce overfitting? Explain in brief.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is a common machine learning problem in which the model fits the training data too closely and fails to generalise effectively to new data.\n",
    "\n",
    " In machine learning, there are numerous methods for reducing overfitting:\n",
    "\n",
    "* Regularization is a strategy for preventing overfitting by introducing a penalty term into the loss function. This penalty term promotes the model to have smaller weights, which reduces the model's complexity.\n",
    "\n",
    "* Cross-validation: Cross-validation is a technique for estimating a model's performance on fresh, unseen data. It entails dividing the data into numerous folds, training the model on a subset of the data, then testing it on the rest of the data.\n",
    "\n",
    "* Early stopping is a strategy for preventing overfitting by terminating the training process before the model has fully converged. This is accomplished by monitoring the model's performance on a validation set and terminating training when the performance begins to deteriorate.\n",
    "\n",
    "* Data augmentation is a strategy for increasing the amount of training data by producing new instances from existing data. This prevents overfitting by giving the model more diverse samples to learn from."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q3: Explain underfitting. List scenarios where underfitting can occur in ML.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a machine learning model is too simplistic to capture the underlying patterns in the data, this is known as underfitting. As a result, the model performs badly on both training and fresh, previously unknown data.\n",
    "\n",
    "Underfitting can occur in a variety of machine learning settings, including:\n",
    "\n",
    "* Inadequate training data: If there is insufficient training data, the model may be unable to capture the underlying patterns in the data, resulting in underfitting.\n",
    "\n",
    "* Oversimplified model: If the model is overly simplistic and lacking in complexity, it may fail to capture the underlying patterns in the data, resulting in underfitting. Using a linear model to fit a non-linear connection between features and the target variable, for example, might result in underfitting.\n",
    "\n",
    "* Data with a lot of random variation: If the data is noisy, the model may not be able to capture the underlying patterns in the data, resulting in underfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a key notion in machine learning that entails balancing a model's complexity against its ability to generalise successfully to new, previously unknown data.\n",
    "\n",
    "\n",
    "According to the bias-variance tradeoff, as the model's complexity rises, the bias lowers but the variance increases. In contrast, when the model's complexity reduces, the bias and variance increase. The objective of machine learning is to get the highest overall performance on fresh, unseen data by finding the correct mix of bias and variance.\n",
    "\n",
    "\n",
    "The difference between the model's average forecast and the real value of the target variable is referred to as bias. High-bias models are extremely simple and fail to reflect the underlying patterns in the data. These models are prone to underfitting the data and performing badly on both training and testing data.\n",
    "\n",
    "Variance, on the other hand, refers to the extent by which the model's predictions fluctuate between training sets. Models with a large variance are extremely complicated and overly close to the training data. These models overfit the data and perform well on training data but badly on fresh, previously unknown data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying overfitting and underfitting in machine learning is critical to ensuring that the model effectively captures the underlying patterns in the data and generalises well to new, previously unknown data. The following are some typical strategies for detecting overfitting and underfitting in machine learning models:\n",
    "\n",
    "* Examining learning curves visually: Learning curves illustrate the model's performance on training and testing data as a function of the number of training samples. The model is likely to be well-fitted if the training and testing curves converge and plateau at a high level. The model may be overfitting if the testing curve is significantly lower than the training curve. If the training and testing curves are both low, the model may be underfitting.\n",
    "\n",
    "* Cross-validation: Cross-validation is a technique for estimating a model's performance on fresh, unseen data. The model may be overfitting if its performance on training data is much better than its performance on validation data. In contrast, if the model performs poorly on both training and validation data, the model may be underfitting.\n",
    "\n",
    "* Regularization analysis: L1 and L2 regularisation techniques can be used to reduce overfitting in machine learning models. The effect of different regularisation strengths on model performance may be used to identify whether the model is overfitting or underfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the expected value of a model's projected output and the real value of the output is referred to as bias. A high bias model is one that is very simplistic and does not adequately match the data. Linear regression models, which presume a linear connection between the input and output variables, and decision trees with few branches, which can only catch simple patterns in the data, are examples of high bias models.\n",
    "\n",
    "Variance, on the other hand, refers to the variation in a model's anticipated output across multiple training sets. A model with a large variance is one that is overly complicated and overfits the training data. Deep neural networks with many layers, which may capture highly complex patterns in data, and decision trees with numerous branches, which can match the training data very closely, are examples of high variance models.\n",
    "\n",
    "High bias models tend to underfit the data and have poor performance on both the training and testing data. High variance models, on the other hand, tend to overfit the data and have good performance on the training data but poor performance on the testing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a machine learning approach that prevents the model from overfitting to the training data. It entails adding a penalty term to the model's loss function to encourage the model to have smaller weights or fewer non-zero coefficients. Regularization minimises the model's complexity and helps to prevent overfitting.\n",
    "\n",
    "L1 regularisation and L2 regularisation are the two most prevalent forms of regularisation algorithms:\n",
    "\n",
    "* L1 regularisation, also known as Lasso regularisation, augments the loss function with a penalty term proportionate to the absolute value of the weights. The penalty term promotes the model to have lower weights and causes some weights to be set to zero. This has the effect of doing feature selection, where the model only predicts the most significant features.\n",
    "\n",
    "* L2 regularisation, also known as Ridge regularisation, modifies the loss function by including a penalty term proportional to the square of the weights. The penalty term promotes the model to have reduced weights, but it does not put any weights to zero, unlike L1 regularisation. It instead diminishes the magnitude of all weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
