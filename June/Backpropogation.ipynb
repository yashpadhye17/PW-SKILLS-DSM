{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the concept of batch normalization in the context of Artificial Neural Networksr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is a technique used in artificial neural networks to improve the training speed and stability of the model. It works by normalizing the inputs of each layer, specifically the activations of the neurons, within a mini-batch of data. \n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. **Normalization**: For each mini-batch of data during training, the mean and standard deviation of the activations for each feature (or neuron) are computed. Then, the activations are normalized by subtracting the mean and dividing by the standard deviation. This centers the data around zero and scales it to have unit variance.\n",
    "\n",
    "2. **Scaling and Shifting**: After normalization, the normalized activations are scaled and shifted using learnable parameters. This step allows the model to undo the normalization if necessary and learn the optimal scaling and shifting for each feature.\n",
    "\n",
    "3. **Stabilizing Training**: Batch normalization helps to stabilize the training process by reducing the internal covariate shift, which is the change in the distribution of the activations of each layer as the parameters of the previous layers change during training. By normalizing the activations, batch normalization reduces the dependence of gradients on the scale of the parameters, making optimization more stable and allowing for the use of higher learning rates.\n",
    "\n",
    "4. **Regularization**: Batch normalization also acts as a form of regularization, similar to dropout. By adding noise to the activations through normalization, batch normalization has a slight regularization effect, which can help prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describe the benefits of using batch normalization during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using batch normalization during training offers several benefits:\n",
    "\n",
    "1. **Improved Training Speed**: Batch normalization accelerates the training process by reducing the internal covariate shift. This allows the use of higher learning rates and speeds up convergence, as the optimization process becomes more stable.\n",
    "\n",
    "2. **Stabilized Gradients**: By normalizing the activations within each mini-batch, batch normalization reduces the dependence of gradients on the scale of the parameters. This helps to alleviate the vanishing or exploding gradient problem, making it easier to train deeper neural networks.\n",
    "\n",
    "3. **Reduction of Overfitting**: Batch normalization acts as a form of regularization by adding noise to the activations through normalization. This helps prevent overfitting, allowing the model to generalize better to unseen data.\n",
    "\n",
    "4. **Removal of Manual Tuning**: Batch normalization reduces the need for manual tuning of hyperparameters such as learning rate and weight initialization. With batch normalization, the network is more robust to different initialization schemes and learning rates, simplifying the training process.\n",
    "\n",
    "5. **Normalization of Activations**: Batch normalization ensures that the activations of each layer have zero mean and unit variance, which can help improve the numerical stability of the network and prevent activations from becoming too large or too small.\n",
    "\n",
    "6. **Facilitation of Deeper Networks**: Batch normalization enables the training of deeper neural networks by providing a stable training signal throughout the network. This allows for the exploration of more complex architectures without encountering issues related to training instability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The working principle of batch normalization involves two main steps: normalization and learnable parameters.\n",
    "\n",
    "1. **Normalization Step**:\n",
    "   - For each mini-batch of data during training, the mean and standard deviation of the activations for each feature (or neuron) are computed. This is typically done along the batch dimension.\n",
    "   - The activations are then normalized by subtracting the mean and dividing by the standard deviation. This centers the data around zero and scales it to have unit variance.\n",
    "\n",
    "2. **Learnable Parameters**:\n",
    "   - After normalization, the normalized activations are scaled and shifted using learnable parameters. These parameters allow the model to undo the normalization if necessary and learn the optimal scaling and shifting for each feature.\n",
    "\n",
    "   \n",
    "By incorporating these two steps, batch normalization ensures that the activations of each layer have zero mean and unit variance while providing the network with the flexibility to learn the optimal scaling and shifting for each feature. This normalization and parameterization process helps stabilize the training process, accelerate convergence, and improve the generalization performance of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Implementation\n",
    "\n",
    "1. Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it\n",
    "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,\n",
    "Tensorlow, xyTorch)\n",
    "3. Train the neural network on the chosen dataset without using batch normalization\n",
    "4. Implement batch normalization layers in the neural network and train the model again\n",
    "5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and\n",
    "without batch normalization\n",
    "6. Discuss the impact of batch normalization on the training process and the performance of the neural\n",
    "network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 3ms/step - loss: 0.2378 - accuracy: 0.9303 - val_loss: 0.1183 - val_accuracy: 0.9630\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1024 - accuracy: 0.9681 - val_loss: 0.0944 - val_accuracy: 0.9699\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0710 - accuracy: 0.9780 - val_loss: 0.0801 - val_accuracy: 0.9754\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0533 - accuracy: 0.9827 - val_loss: 0.0779 - val_accuracy: 0.9775\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0430 - accuracy: 0.9862 - val_loss: 0.0763 - val_accuracy: 0.9778\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2024 - accuracy: 0.9384 - val_loss: 0.1058 - val_accuracy: 0.9677\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1023 - accuracy: 0.9683 - val_loss: 0.0803 - val_accuracy: 0.9741\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0765 - accuracy: 0.9764 - val_loss: 0.0740 - val_accuracy: 0.9766\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0643 - accuracy: 0.9796 - val_loss: 0.0805 - val_accuracy: 0.9759\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0550 - accuracy: 0.9821 - val_loss: 0.0718 - val_accuracy: 0.9779\n",
      "Model without Batch Normalization:\n",
      "Train Accuracy: 0.9861999750137329\n",
      "Validation Accuracy: 0.9778000116348267\n",
      "\n",
      "Model with Batch Normalization:\n",
      "Train Accuracy: 0.9820500016212463\n",
      "Validation Accuracy: 0.9779000282287598\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the dataset (using MNIST as an example)\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Define the neural network without batch normalization\n",
    "def create_model_without_batch_norm():\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the neural network with batch normalization\n",
    "def create_model_with_batch_norm():\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_images, train_labels, test_images, test_labels, epochs):\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss_function,\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(train_images, train_labels, epochs=epochs, validation_data=(test_images, test_labels))\n",
    "    return history\n",
    "\n",
    "# Create and train the model without batch normalization\n",
    "model_without_batch_norm = create_model_without_batch_norm()\n",
    "history_without_batch_norm = train_model(model_without_batch_norm, train_images, train_labels, test_images, test_labels, epochs=5)\n",
    "\n",
    "# Create and train the model with batch normalization\n",
    "model_with_batch_norm = create_model_with_batch_norm()\n",
    "history_with_batch_norm = train_model(model_with_batch_norm, train_images, train_labels, test_images, test_labels, epochs=5)\n",
    "\n",
    "# Compare training and validation performance\n",
    "print(\"Model without Batch Normalization:\")\n",
    "print(\"Train Accuracy:\", history_without_batch_norm.history['accuracy'][-1])\n",
    "print(\"Validation Accuracy:\", history_without_batch_norm.history['val_accuracy'][-1])\n",
    "\n",
    "print(\"\\nModel with Batch Normalization:\")\n",
    "print(\"Train Accuracy:\", history_with_batch_norm.history['accuracy'][-1])\n",
    "print(\"Validation Accuracy:\", history_with_batch_norm.history['val_accuracy'][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. Experimentation\n",
    "1. Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization offers several advantages in improving the training of neural networks:\n",
    "\n",
    "1. **Stabilized Training**: Batch normalization reduces internal covariate shift by normalizing the activations within each mini-batch. This stabilizes the training process, making it less sensitive to the initialization of weights and allowing for faster convergence.\n",
    "\n",
    "2. **Accelerated Training**: By mitigating the vanishing or exploding gradient problem, batch normalization enables the use of higher learning rates. This accelerates the training process, as the model can make larger updates to its parameters, leading to faster convergence.\n",
    "\n",
    "3. **Improved Gradient Flow**: Normalizing the activations helps maintain a more consistent gradient flow during backpropagation. This leads to more stable gradients and smoother optimization trajectories, making it easier to train deeper neural networks.\n",
    "\n",
    "4. **Regularization Effect**: Batch normalization has a slight regularization effect due to the noise introduced during the normalization process. This helps prevent overfitting, allowing the model to generalize better to unseen data without relying solely on techniques like dropout.\n",
    "\n",
    "5. **Robustness to Parameter Initialization**: Batch normalization makes neural networks less sensitive to the choice of initialization for the model parameters. This allows for more straightforward training, as the need for careful initialization schemes is reduced.\n",
    "\n",
    "Despite its advantages, batch normalization also has some potential limitations:\n",
    "\n",
    "1. **Increased Computational Overhead**: Batch normalization introduces additional computations during both training and inference, which can increase the overall computational cost of training neural networks.\n",
    "\n",
    "2. **Dependency on Batch Size**: The performance of batch normalization can be sensitive to the batch size used during training. Smaller batch sizes may lead to less stable normalization statistics, while larger batch sizes might not capture the fine-grained statistics of the data effectively.\n",
    "\n",
    "3. **Difficulty in Deploying Pre-trained Models**: Batch normalization requires maintaining the statistics of the mean and variance for each feature during training, which makes it challenging to deploy pre-trained models directly without access to the training data.\n",
    "\n",
    "4. **Limited Applicability in Certain Architectures**: While batch normalization is widely used in feedforward neural networks, its applicability in recurrent neural networks (RNNs) and generative models like Generative Adversarial Networks (GANs) may be limited due to issues with sequence dependencies and mode collapse, respectively.\n",
    "\n",
    "Overall, batch normalization is a powerful technique for improving the training of neural networks, but it's essential to consider its computational overhead and sensitivity to batch size when applying it in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
